# DDPG 与 TD3 算法

本章开始到接下来的几章，我们将介绍一些非常典型的基于策略梯度的算法，包括 `DDPG`、`PPO`、`SAC` 等。这些方法的实现方式各不相同，也各有各的特点，因此每个算法都单独形成一个章节来展开。同时它们是目前实践中最最常用的一些算法，甚至有 “遇事不决PPO” 之类的说法，因此读者一定要认真学习。那么本章将 `DDPG` 算法作为开篇，严格来说，原论文作者提出 `DDPG` 算法的初衷其实是 `DQN` 算法的一个扩展，或者说一种连续动作版本的 `DQN` 算法。但是由于当时 `Actor-Critic` 架构还没有被提出（`A3C` 算法是在 2016 年发表的，比 `DDPG` 算法晚了一年），只是后来我们回看 `DDPG` 算法的时候发现其在形式上更像 `Actor-Critic` 的架构，因此我们就将其归为 `Actor-Critic` 算法的一种。
## DPG 方法

`DDPG` 算法，英文全称为 `Deep Deterministic Policy Gradient` ，中文全称为深度确定性策略梯度算法，是一种确定性的策略梯度算法。为了让读者更好地理解 `DDPG` 算法，我们先把“深度”这两个字去掉，即先介绍一下 `DPG` 算法，后面我们也会发现 `DPG` 算法其实就是 `DDPG` 算法的核心，也就是精髓所在。

虽然 `DDPG` 原论文作者是基于 `DQN` 算法来展开的，目前很多相关资料也是如此。但是笔者认为，有了 `Actor-Critic` 算法的铺垫之后，从策略梯度的角度来理解 DPG 算法是更容易的。首先我们知道 `DQN` 算法的一个主要缺点就是不能用于连续动作空间，这是因为在 `DQN` 算法中动作是通过贪心策略或者说 `argmax` 的方式来从 `Q` 函数间接得到的。要想适配连续动作空间，我们干脆就将选择动作的过程变成一个直接从状态映射到具体动作的函数 $\mu_\theta (s)$，其中 $\theta$ 表示模型的参数，这样一来就把求解 `Q` 函数、贪心选择动作这两个过程合并成了一个函数，也就是我们常说的 `Actor`。有读者可能会问，为什么这里用 $\mu_\theta (s)$ 来表示策略而不是 `Actor-Critic` 章节中提到的 $\pi_{\theta}(a|s)$ 呢？注意，这里 $\mu_\theta (s)$ 输出的是一个值，而 $\pi_{\theta}(a|s)$ 通常输出的是一个概率分布，这是两者的本质区别，与输出概率分布的随机性策略（`Stochastic Policy`）不同，这里输出一个值的策略我们就称作确定性策略（`Deterministic Policy`）。有了这个策略函数之后，类似地，我们也可以推导出 `DPG` 算法的目标函数，也就是策略梯度公式，如下：

$$
\nabla_\theta J(\theta) \approx \mathbb{E}_{s_t \sim \rho^\beta}\left[\left.\nabla_a Q\left(s_t, a\right)\right|_{a=\mu_\theta\left(s_t\right)} \nabla_\theta \mu_\theta\left(s_t\right)\right]
$$

其中 $\rho^\beta$ 是策略的初始分布，用于探索状态空间，在实际应用中相当于网络模型的初始参数，读者可以不用关注。另外注意这里的 $Q(s_t, a)$ 表示的不是 `Q`函数，跟 `Actor-Critic` 算法一样，即作为一个 `Critic` 网络，将状态和动作作为输入，并且输出一个值。`DDPG` 策略梯度公式的推导过程与 `Actor-Critic` 算法的推导过程虽然有所区别，但整体上是非常相似的，感兴趣的读者可以查询相关资料或者自行推导一下，这里就不再赘述了。

## DDPG 算法

有了 `DPG` 算法的铺垫之后，我们再来看看 `DDPG` 算法。`DDPG` 算法的 “深度” 其实很简单，它就体现在于 `DPG` 算法中使用 `Actor` 和 `Critic` 两个网络来分别表示策略函数（确定性策略）和值函数，它们可以是线性模型或浅层神经网络。而DDPG算法采用深度神经网络来近似 `Actor` 和`Critic`，使得算法可以处理更复杂的任务和高维的状态空间。用现在更流行的话讲，`DDPG` 算法同 `DPG` 算法相比，就是使用了更大的模型。除此之外，`DDPG` 算法还有一些其他的改进，例如使用了经验回放、目标网络等 `DQN` 算法中的一些技巧。因此，`DDPG` 算法更像是将 `DPD` 思路引入到了 `DQN` 算法中，而不是简单的将 `DPG` 算法扩展到了深度神经网络上。另外，`DDPG` 算法还有一项重要的贡献，就是引入了噪声网络来增加策略的探索性，即在训练过程中采用了一种确定性策略加噪声的方式。即在Actor网络的输出动作上加入一定的噪声，使得策略在训练过程中具有一定的探索性，有助于探索更广泛的动作空间。

这里读者顺便插一句题外话，回顾到目前为止的所有章节内容，不知道读者有没有发现，其实我们发现在强化学习基础算法的研究改进当中，无外乎几个亘古不变的主题：一是**如何提高对值函数的估计**，保证其准确性，即尽量无偏且低方差，例如最开始的用深度神经网络替代简单的 `Q` 表、结合蒙特卡洛和时序差分的 $TD(\lambda)$ 、引入目标网络以及广义优势估计等等；二是**如何提高探索以及平衡探索-利用的问题**，尤其在探索性比较差的确定性策略中，例如`DQN`和`DDPG`算法都会利用各种技巧来提高探索，例如经验回放、$\varepsilon$-greedy 策略、噪声网络等等。这两个问题是强化学习算法的基础核心问题，希望能够给读者在学习和研究的过程中带来一定的启发。

回到正题，由于目标网络、经验回放前面章节都讲过了，这里就略过，我们讲讲 `DDPG` 引入的噪声。其实引入噪声的方法在前面 `Noisy DQN` 算法中就讲到了，只是 `Noisy DQN` 算法是在网络中引入噪声，而 `DDPG` 算法是在输出动作上引入噪声。本质上来讲，引入噪声的作用就是为了在不破坏系统的前提下，提高系统运行的抗干扰性。这跟我们生活中打疫苗是类似的，通常我们会将灭活的病毒也就是疫苗注入到体内，引发免疫系统的警觉，从而提高免疫系统的抗干扰性，即提高我们身体的免疫力。这里疫苗就相当于轻微的噪声，如果免疫系统一直没见过这种噪声，那么一旦遇到真正的病毒之后是很有可能崩溃的，反之如果经常接触这种轻微的噪声，那么免疫系统就会逐渐适应，从而提高抗干扰性。又好比我们平时做的消防演练，虽然平时的演练都不是真正意义上的灾害，但经过熟练的演练之后一旦遇到真正的灾害不说从容应对，至少也不会过于慌乱了。再次回归正题，`DDPG` 算法是在输出动作上引入噪声的，由于 $\mu_\theta (s)$ 输出的是单个值，其实最简单的方式就是在输出的值上加上一个随机数，这个随机数可以是正态分布的（即高斯噪声），也可以是均匀分布的，只要能够保证这个随机数的值不要过大就行。

当然简单的噪声引入除了简单这一个优点之外，可能剩下的全都是缺点了，因此在 `DDPG` 算法中使用的其实是一种叫做 `Ornstein-Uhlenbeck` 的噪声，简称 `OU` 噪声。`OU` 噪声是一种具有回归特性的随机过程，其与高斯噪声相比的优点在于：

* **探索性**：`OU` 噪声具有持续的、自相关的特性。相比于独立的高斯噪声，`OU` 噪声更加平滑，并且在训练过程中更加稳定。这种平滑特性使得OU噪声有助于探索更广泛的动作空间，并且更容易找到更好的策略。
* **控制幅度**：`OU` 噪声可以通过调整其参数来控制噪声的幅度。在 `DDPG` 算法中，可以通过调整 `OU` 噪声的方差来控制噪声的大小，从而平衡探索性和利用性。较大的方差会增加探索性，而较小的方差会增加利用性。
* **稳定性**：`OU` 噪声的回归特性使得噪声在训练过程中具有一定的稳定性。相比于纯粹的随机噪声，在 `DDPG` 算法中使用`OU` 噪声可以更好地保持动作的连续性，避免剧烈的抖动，从而使得训练过程更加平滑和稳定。
* **可控性**：由于`OU` 噪声具有回归特性，它在训练过程中逐渐回归到均值，因此可以控制策略的探索性逐渐减小。这种可控性使得在训练的早期增加探索性，然后逐渐减小探索性，有助于更有效地进行训练。

总的来说，`OU` 噪声作为 `DDPG` 算法中的一种探索策略，具有平滑、可控、稳定等优点，使得算法能够更好地在连续动作空间中进行训练，探索更广泛的动作空间，并找到更优的策略。它是 `DDPG` 算法成功应用于连续动作空间问题的重要因素之一。虽然它有这么多的优点，实际上在简单的环境中，它跟使用简单的高斯噪声甚至不用噪声的效果是差不多的，只有在复杂的环境中才会体现出来区别。因此，如果读者在实际应用中面临的问题比较简单，可以不用OU噪声，而是使用高斯噪声或者不用噪声即可，这样可以减少算法的复杂度，加快算法的收敛速度，正所谓 “杀鸡焉用牛刀”。

`OU` 噪声主要由两个部分组成：随机高斯噪声和回归项，其数学定义如下：

$$
d x_t=\theta\left(\mu-x_t\right) d t+\sigma d W_t
$$

其中 $x_t$ 是OU过程在时间 $t$ 的值，即当前的噪声值，这个 $t$ 也是强化学习中的时步 （time step）。$\mu$ 是回归到的均值，表示噪声在长时间尺度上的平均值。$\theta$ 是 `OU` 过程的回归速率，表示噪声向均值回归的速率。$\sigma$ 是 `OU` 过程的扰动项，表示随机高斯噪声的标准差。$dW_t$ 是布朗运动（Brownian motion）或者维纳过程（Wiener process），是一个随机项，表示随机高斯噪声的微小变化。在实际应用中，我们只需要调整 $\mu$ 和 $\sigma$ 就可以了，$\theta$ 通常是固定的，而 $dW_t$ 是随机项，我们也不需要关注。尽管如此，需要调整的参数还是有点多，这也是为什么 `DDPG` 算法的调参比较麻烦的原因之一。

## DDPG 算法的优缺点

总的来说，`DDPG` 算法的优点主要有：

* **适用于连续动作空间**：DDPG算法采用了确定性策略来选择动作，这使得它能够直接处理连续动作空间的问题。相比于传统的随机策略，确定性策略更容易优化和学习，因为它不需要进行动作采样，h缓解了在连续动作空间中的高方差问题。
* **高效的梯度优化**：DDPG算法使用策略梯度方法进行优化，其梯度更新相对高效，并且能够处理高维度的状态空间和动作空间。同时，通过Actor-Critic结构，算法可以利用值函数来辅助策略的优化，提高算法的收敛速度和稳定性。
* **经验回放和目标网络**：这是老生常谈的内容了，经验回放机制可以减少样本之间的相关性，提高样本的有效利用率，并且增加训练的稳定性。目标网络可以稳定训练过程，避免值函数估计和目标值之间的相关性问题，从而提高算法的稳定性和收敛性。

而缺点在于：

* **只适用于连续动作空间**：这既是优点，也是缺点。

* **高度依赖超参数**：`DDPG` 算法中有许多超参数需要进行调整，除了一些 `DQN`的算法参数例如学习率、批量大小、目标网络的更新频率等，还需要调整一些 `OU` 噪声的参数
调整这些超参数并找到最优的取值通常是一个挑战性的任务，可能需要大量的实验和经验。

* **高度敏感的初始条件**：`DDPG` 算法对初始条件非常敏感。初始策略和值函数的参数设置可能会影响算法的收敛性和性能，需要仔细选择和调整。

* 容易陷入局部最优：由于采用了确定性策略，可能会导致算法陷入局部最优，难以找到全局最优策略。为了增加探索性，需要采取一些措施，如加入噪声策略或使用其他的探索方法。

可以看到，`DDPG` 算法的优点可能掩盖不了它众多的缺点，尤其对于初学者来说，调参是一个非常头疼的问题，因此在实际应用中，同样情况下可能会选择更加简单的 `PPO` 算法会来得更加容易一些。当然，对于一些熟练的调参大侠来说，`DDPG` 算法以及相关改进版本的算法也是值得尝试的，毕竟它们在实际应用中的效果还是非常不错的。

## TD3 算法

我们知道 `DDPG` 算法的缺点太多明显，因此后来有人对其进行了改进，这就是我们接下来要介绍的 `TD3` 算法。`TD3` 算法，英文全称为 `Twin Delayed DDPG`，中文全称为双延迟确定性策略梯度算法。相对于 `DDPG` 算法，`TD3` 算法的改进主要做了两点重要的改进，一是 双 `Q` 网络，体现在名字中的 `Twin`，二是 延迟更新，体现在名字中的 `Delayed`。这两点改进都是为了解决 `DDPG` 算法中的一些问题，下面我们分别来看一下。

## 实战：DDPG 算法

## 实战：TD3 算法
