# DDPG 算法

本章开始到接下来的几章，我们将介绍一些非常典型的基于策略梯度的算法，包括 `DDPG`、`PPO`、`SAC` 等。这些方法的实现方式各不相同，也各有各的特点，因此每个算法都单独形成一个章节来展开。同时它们是目前实践中最最常用的一些算法，甚至有 “遇事不决PPO” 之类的说法，因此读者一定要认真学习。那么本章将 `DDPG` 算法作为开篇，严格来说，原论文作者提出 `DDPG` 算法的初衷其实是 `DQN` 算法的一个扩展，或者说一种连续动作版本的 `DQN` 算法。但是由于当时 `Actor-Critic` 架构还没有被提出（`A3C` 算法是在 2016 年发表的，比 `DDPG` 算法晚了一年），只是后来我们回看 `DDPG` 算法的时候发现其在形式上更像 `Actor-Critic` 的架构，因此我们就将其归为 `Actor-Critic` 算法的一种。
## DPG 方法

`DDPG` 算法，英文全称为 `Deep Deterministic Policy Gradient` ，中文全称为深度确定性策略梯度算法，是一种确定性的策略梯度算法。为了让读者更好地理解 `DDPG` 算法，我们先把“深度”这两个字去掉，即先介绍一下 `DPG` 算法，后面我们也会发现 `DPG` 算法其实就是 `DDPG` 算法的核心，也就是精髓所在。

虽然 `DDPG` 原论文作者是基于 `DQN` 算法来展开的，目前很多相关资料也是如此。但是笔者认为，有了 `Actor-Critic` 算法的铺垫之后，从策略梯度的角度来理解 DPG 算法是更容易的。首先我们知道 `DQN` 算法的一个主要缺点就是不能用于连续动作空间，这是因为在 `DQN` 算法中动作是通过贪心策略或者说 `argmax` 的方式来从 `Q` 函数间接得到的。要想适配连续动作空间，我们干脆就将选择动作的过程变成一个直接从状态映射到具体动作的函数 $\mu_\theta (s)$，其中 $\theta$ 表示模型的参数，这样一来就把求解 `Q` 函数、贪心选择动作这两个过程合并成了一个函数，也就是我们常说的 `Actor`。有读者可能会问，为什么这里用 $\mu_\theta (s)$ 来表示策略而不是 `Actor-Critic` 章节中提到的 $\pi_{\theta}(a|s)$ 呢？注意，这里 $\mu_\theta (s)$ 输出的是一个值，而 $\pi_{\theta}(a|s)$ 通常输出的是一个概率分布，这是两者的本质区别，与输出概率分布的随机性策略（`Stochastic Policy`）不同，这里输出一个值的策略我们就称作确定性策略（`Deterministic Policy`）。有了这个策略函数之后，类似地，我们也可以推导出 `DPG` 算法的目标函数，也就是策略梯度公式，如下：

$$
\nabla_\theta J(\theta) \approx \mathbb{E}_{s_t \sim \rho^\beta}\left[\left.\nabla_a Q\left(s_t, a\right)\right|_{a=\mu_\theta\left(s_t\right)} \nabla_\theta \mu_\theta\left(s_t\right)\right]
$$

其中 $\rho^\beta$ 是策略的初始分布，用于探索状态空间，在实际应用中相当于网络模型的初始参数，读者可以不用关注。另外注意这里的 $Q(s_t, a)$ 表示的不是 `Q`函数，跟 `Actor-Critic` 算法一样，即作为一个 `Critic` 网络，将状态和动作作为输入，并且输出一个值。`DDPG` 策略梯度公式的推导过程与 `Actor-Critic` 算法的推导过程虽然有所区别，但整体上是非常相似的，感兴趣的读者可以查询相关资料或者自行推导一下，这里就不再赘述了。

## DDPG 算法

有了 `DPG` 算法的铺垫之后，我们再来看看 `DDPG` 算法。`DDPG` 算法的 “深度” 其实很简单，它就体现在于 `DPG` 算法中使用 `Actor` 和 `Critic` 两个网络来分别表示策略函数（确定性策略）和值函数，它们可以是线性模型或浅层神经网络。而DDPG算法采用深度神经网络来近似 `Actor` 和`Critic`，使得算法可以处理更复杂的任务和高维的状态空间。用现在更流行的话讲，`DDPG` 算法同 `DPG` 算法相比，就是使用了更大的模型。除此之外，`DDPG` 算法还有一些其他的改进，例如使用了经验回放、目标网络等 `DQN` 算法中的一些技巧。因此，`DDPG` 算法更像是将 `DPD` 思路引入到了 `DQN` 算法中，而不是简单的将 `DPG` 算法扩展到了深度神经网络上。另外，`DDPG` 算法还有一项重要的贡献，就是引入了噪声网络来增加策略的探索性，即在训练过程中采用了一种确定性策略加噪声的方式。即在Actor网络的输出动作上加入一定的噪声，使得策略在训练过程中具有一定的探索性，有助于探索更广泛的动作空间。

这里读者顺便插一句题外话，回顾到目前为止的所有章节内容，不知道读者有没有发现，其实我们发现在强化学习基础算法的研究改进当中，无外乎几个不变的主题：一是如何提高对值函数的估计，保证其准确性，即尽量无偏且低方差，例如最开始的用深度神经网络替代简单的 `Q` 表、结合蒙特卡洛和时序差分的 $TD(\lambda)$ 、引入目标网络以及广义优势估计等等；二是如何提高探索以及平衡探索-利用的问题，尤其在探索性比较差的确定性策略中，例如`DQN`和`DDPG`算法都会利用各种技巧来提高探索，例如经验回放、$\varepsilon$-greedy 策略、噪声网络等等。这两个问题是强化学习算法的基础核心问题，希望能够给读者在学习和研究的过程中带来一定的启发。