# 深度学习基础

$\qquad$ 在前面章节中我们主要介绍了传统强化学习的内容，这些内容涵盖了基础的问题核心和解决方法。但是对应的算法并不能解决高维度的复杂问题，因此现在普遍流行将深度学习和强化学习结合起来，利用深度学习网络强大的拟合能力通过将状态、动作等作为输入，来估计对应的状态价值和动作价值等等。在过渡到深度强化学习之前，本章将对强化学习中用到的一些深度学习知识（主要包括各种神经网络等）作一个简要归纳。这些归纳主要面向已经有一些深度学习基础的读者，如果读者对深度学习还不是很熟悉，可以对照本章的内容，去学习由笔者共同著作的另外一本书《李宏毅深度学习笔记》。

## 强化学习与深度学习的关系

$\qquad$ 之前我们讲到了强化学习的问题可以拆分成两类问题，即预测和控制。预测的主要目的是根据环境的状态和动作来预测状态价值和动作价值，而控制的主要目的是根据状态价值和动作价值来选择动作。换句话说，预测主要是告诉我们当前状态下采取什么动作比较好，而控制则是按照某种方式决策。就好比军师与主公的关系，军师提供他认为最佳的策略，而主公则决定是否采纳这个策略。不知道读者们是否看过《超智能足球$\text{GGO}$》这部热血动漫，老实讲它是笔者看过比较好的带有高科技元素的足球动漫，主要讲述的是主角团带领着他们的超智能足球机器人组队打入世界大赛的故事，也是启引笔者选择强化学习的初衷之一。

$\qquad$ 如图 $\text{4-1}$ 所示，其中有一队叫做英国三狮，主要领队是尼尔逊和巴菲斯，巴菲斯是一个超级数据分析专家，他能在各种场景下计算对手传球、射门的概率，也包括我方进球和传球的各种收益，然后尼尔逊会根据他的数据分析结果来决定下一步行动。尼尔逊也是一个非常有头脑的领队，他不会只依靠巴菲斯的计算结果，而是会结合自身的经验和对足球的直觉来做出数据之外的决策。这个数据之外的决策在强化学习中叫做探索，也就是说尼尔逊会根据巴菲斯的计算结果来做出决策，但是他也会根据自己的经验和直觉来做出一些不确定的决策，这样才能保证他的队伍不会被对手轻易的猜到。

<div align=center>
<img width="400" src="../figs/ch6/ggo.png"/>
</div>
<div align=center>图 $\text{4-1}$ 迷你网格示例</div>

$\qquad$ 以上就是预测和控制的关系，通常在强化学习中预测和控制的部分看起来是共用一个 $Q$ 表或者神经网络的，因此读者们可能会因为主要关注价值函数的估计而忽视掉控制这层关系，控制通常在采样动作的过程中体现出来。其实在前面也提到过，预测也相当于人的眼睛和大脑的视觉神经处理部分，而控制相当于大脑的决策神经处理部分，看似是两个独立的部分，但实际上是相互依赖的，预测的结果会影响到控制的决策，而控制的决策也会影响到预测的结果。


## 线性模型

首先介绍线性模型，线性模型是最简单的一类机器学习模型，可以将其视为单层的神经网络。在线性模型中最基础的两个模型就是线性回归和逻辑回归，通常分别用于解决回归和分类问题，尽管后者也可以用来解决回归问题。回归模型的输出是一个连续的值，而分类模型的输出是离散的值，用于表示分类。本质上来说回归模型和分类模型是一样的，分类模型可以将回归模型离散化，例如本节将要讲的逻辑回归就是在线性回归的基础上增加了一个 sigmoid 函数对其进行了离散化。顺便提一句，回归模型也可以将分类模型连续化，通常见于贝叶斯模型中，但这在强化学习中并不常用。

**线性回归**。以Kaggle入门竞赛项目房价预测为例，一套房子有$m$个特征，例如建造年份、房子面积等等，把这$m$个特征用向量表示，如下：

$$
\boldsymbol{x}=\left[x_1, x_2, \cdots, x_m\right]
$$

我们可以用线性模型来拟合这$m$个特征和房价的关系，如下：

$$
f(\boldsymbol{x} ; \boldsymbol{w}, b) = w_1 x_1+w_2 x_2+\cdots+w_m x_m+b = \boldsymbol{w}^T \boldsymbol{x}+b
$$

其中$\boldsymbol{w}$和$b$是模型的参数，$f(\boldsymbol{x} ; \boldsymbol{w}, b)$是模型的输出，也就是我们要预测的房价。出于简化考虑，通常我们会用一个符号$\boldsymbol{\theta}$来表示$\boldsymbol{w}$和$b$，如下：

$$
f^{\theta}(\boldsymbol{x}) = \boldsymbol{\theta}^T \boldsymbol{x}
$$

我们的目的是求得一组最优的参数$\boldsymbol{\theta^{*}}$，使得该模型能够根据房屋的$m$个特征预测对应的房价。我们一般利用历史数据来近似求解最优参数，这个过程就叫做训练。注意这里是近似求解，因为几乎所有机器学习模型都无法找到一种方法能够获得绝对的最优解，甚至也不一定存在绝对最优解，有些方法甚至很容易陷入局部最小值的问题中。训练的方法，或者说求解模型参数的方法理论上来说有很多种，比如这里线性模型可以用最小二乘法来求解，另外有些模型可以用牛顿法来求解，而目前普遍流行的优化方法就是梯度下降。梯度下降方法泛化能力很强，能够基于梯度下降求解很多种模型，该方法的本质是一阶泰勒展开，顺便提一句，牛顿法则是二阶泰勒展开。

**逻辑回归**。对于分类问题，其预测目标不再是连续的值，而可能是二元变量，要么等于0，要么等于1，即最简单的二分类问题。这种情况下，我们可以用逻辑回归来解决，注意虽然逻辑回归名字里带有回归，但通常用于解决二分类问题而非回归问题。逻辑回归的思路也比较简单，如图 6.1 所示，就是在线性模型的后面增加一个sigmoid函数，我们一般称之为激活函数。逻辑回归模型其实可以看做神经网络模型的一个神经元，具体后面再展开说明。

<div align=center>
<img width="800" src="../figs/ch6/logistic_struction.png"/>
</div>
<div align=center>图 6.1 逻辑回归结构</div>

sigmoid 函数定义为：

$$
sigmoid(z) = \frac{1}{1+exp(-z)}
$$

如图 6.2 所示，sigmoid 函数可以将输入的任意实数映射到$0-1$之间，对其输出的值进行判断，例如小于0.5我们认为预测的是类别0，反之是类别1，这样一来通过梯度下降来求解模型参数就可以用于实现二分类问题了。注意，虽然逻辑回归只是在线性回归模型基础上增加了一个激活函数，但两个模型是完全不同的，包括损失函数等等。线性回归的损失函数是均方差损失，而逻辑回归模型一般是交叉熵损失，这两种损失函数在深度学习和深度强化学习中都很常见，具体推导细节感兴趣的读者可自行翻阅相关资料。

<div align=center>
<img width="400" src="../figs/ch6/sigmoid.png"/>
</div>
<div align=center>图 6.2 Sigmoid函数图像</div>

## 神经网络

### 全连接网络

全连接网络又称作**多层感知机（multi-layer perceptron，MLP）**，是最基础的神经网络模型。它是基于生物神经网络的启发，将“线性函数+激活函数”这样的结构一层层堆叠（stack）组合成一个多层的网络模型，用于解决更复杂的问题。如\figref{fig:ann_vs_dnn}所示，线性函数可以看做生物神经网络的神经元，而激活函数就是神经元之间的突触结构。顺便说一句，生物神经网络是神奇且复杂的，人们也一直在尝试研究新的人工神经网络模型去模拟生物神经网络，例如脉冲神经网络，尽管目前这些模型还没有得到广泛地验证。

<div align=center>
<img width="800" src="../figs/ch6/ann_vs_dnn.png"/>
</div>
<div align=center>图 6.3 生物神经网络与人工神经网络的对比</div>

记神经网络模型中上一层的输入向量为$\boldsymbol{x^{l-1}}\in \mathbb{R}^{d^{l-1}}$，其中第一层的输入也就是整个模型的输入可记为$\boldsymbol{x^0}$，每一个全连接层将上一层的输入映射到$\boldsymbol{x^{l}}\in \mathbb{R}^{d^{l}}$，也就是下一层的输入，具体定义为：

$$
\boldsymbol{x}^{l}=\sigma(\boldsymbol{z}), \quad \boldsymbol{z}=\boldsymbol{W} \boldsymbol{x^{l-1}}+\boldsymbol{b} = \boldsymbol{\theta} \boldsymbol{x^{l-1}}
$$

其中$\boldsymbol{W}\in \mathbb{R}^{d^{l-1} \times d^{l}}$是权重矩阵，$\boldsymbol{b}$为偏置矩阵，与线性模型类型，这两个参数我们通常看作一个参数$\boldsymbol{\theta}$。$\sigma(\cdot)$是激活函数，除了 Sigmoid 函数之外，还包括 Softmax 函数、ReLU 函数和 tanh 函数等等激活函数。其中最常用的是 ReLU 函数 和 tanh 函数，前者将神经元也就是线性函数的输出映射到$0-1$之间，后者则映射到$-1$到$1$之间。前面讲到，在强化学习中我们用神经网络来近似动作价值函数，动作价值函数的输入是状态，输出是各个动作对应的价值，在有些连续动作问题中比如汽车方向盘转动角度是$-90$度到$90$度之间，这种情况下使用 tanh 激活函数能够使得神经网络负值以便于更好地近似状态动作函数。顺便提一句，这里还有一种做法是我们可以把动作空间映射到正值的范围，例如$0$到$180$区间，这样一来对应的神经网络模型激活函数使用 ReLU 函数会更好些。

一个$l$层的神经网络模型可以表示为：
$$
\begin{split}
    第 1 层: \quad \boldsymbol{x}^{(1)}=\sigma_1\left(\boldsymbol{W}^{(1)} \boldsymbol{x}^{(0)}+\boldsymbol{b}^{(1)}\right),\\
    第 2 层: \quad \boldsymbol{x}^{(2)}=\sigma_2\left(\boldsymbol{W}^{(2)} \boldsymbol{x}^{(1)}+\boldsymbol{b}^{(2)}\right),\\
    \vdots \quad \vdots\\
    第 l 层: \quad \boldsymbol{x}^{(l)}=\sigma_l\left(\boldsymbol{W}^{(l)} \boldsymbol{x}^{(l-1)}+\boldsymbol{b}^{(l)}\right)\\
\end{split}
$$

求解神经网络模型参数的方法除了梯度下降之外，还涉及多层网络模型的正向传播和反向传播，具体细节在接下来的小节中展开。

### 卷积神经网络

### 循环神经网络

### Transformer

## 梯度下降

## 反向传播
