# 深度学习基础

$\qquad$ 在前面章节中我们主要介绍了传统强化学习的内容，这些内容涵盖了基础的问题核心和解决方法。但是对应的算法并不能解决高维度的复杂问题，因此现在普遍流行将深度学习和强化学习结合起来，利用深度学习网络强大的拟合能力通过将状态、动作等作为输入，来估计对应的状态价值和动作价值等等。在过渡到深度强化学习之前，本章将对强化学习中用到的一些深度学习知识（主要包括各种神经网络等）作一个简要归纳。这些归纳主要面向已经有一些深度学习基础的读者，如果读者对深度学习还不是很熟悉，可以对照本章的内容，去学习由笔者共同著作的另外一本书《李宏毅深度学习笔记》。

## 强化学习与深度学习的关系

$\qquad$ 之前我们讲到了强化学习的问题可以拆分成两类问题，即预测和控制。预测的主要目的是根据环境的状态和动作来预测状态价值和动作价值，而控制的主要目的是根据状态价值和动作价值来选择动作。换句话说，预测主要是告诉我们当前状态下采取什么动作比较好，而控制则是按照某种方式决策。就好比军师与主公的关系，军师提供他认为最佳的策略，而主公则决定是否采纳这个策略。不知道读者们是否看过《超智能足球$\text{GGO}$》这部热血动漫，老实讲它是笔者看过比较好的带有高科技元素的足球动漫，主要讲述的是主角团带领着他们的超智能足球机器人组队打入世界大赛的故事，也是启引笔者选择强化学习的初衷之一。

$\qquad$ 如图 $\text{6-1}$ 所示，其中有一队叫做英国三狮，主要领队是尼尔逊和巴菲斯，巴菲斯是一个超级数据分析专家，他能在各种场景下计算对手传球、射门的概率，也包括我方进球和传球的各种收益，然后尼尔逊会根据他的数据分析结果来决定下一步行动。尼尔逊也是一个非常有头脑的领队，他不会只依靠巴菲斯的计算结果，而是会结合自身的经验和对足球的直觉来做出数据之外的决策。这个数据之外的决策在强化学习中叫做探索，也就是说尼尔逊会根据巴菲斯的计算结果来做出决策，但是他也会根据自己的经验和直觉来做出一些不确定的决策，这样才能保证他的队伍不会被对手轻易的猜到。

<div align=center>
<img width="400" src="../figs/ch6/ggo.png"/>
</div>
<div align=center>图 $\text{6-1}$ 预测与控制示例</div>

$\qquad$ 以上就是预测和控制的关系，通常在强化学习中预测和控制的部分看起来是共用一个 $Q$ 表或者神经网络的，因此读者们可能会因为主要关注价值函数的估计而忽视掉控制这层关系，控制通常在采样动作的过程中体现出来。其实在前面也提到过，预测也相当于人的眼睛和大脑的视觉神经处理部分，而控制相当于大脑的决策神经处理部分，看似是两个独立的部分，但实际上是相互依赖的，预测的结果会影响到控制的决策，而控制的决策也会影响到预测的结果。

$\qquad$ 讲到这里，读者应该不难理解，深度学习就是用来提高强化学习中预测的效果的，因为深度学习本身就是一个目前预测和分类效果俱佳的工具。比如 $\text{Q-learning}$ 的 $Q$ 表就完全可以用神经网络来拟合。注意，深度学习只是一种非常广泛的应用，但并不是强化学习的必要条件，也可以是一些传统的预测模型，例如决策树、贝叶斯模型等等，因此读者在研究相关问题时需要充分打开思路。类似地，在控制问题中，也可以利用深度学习或者其他的方法来提高性能，例如结合进化算法来提高强化学习的探索能力。

$\qquad$ 从训练模式上来看，深度学习和强化学习，尤其是结合了深度学习的深度强化学习，都是基于**大量的样本**来对相应算法进行迭代更新并且达到最优的，这个过程我们称之为**训练**。但与另外两者不同的是，强化学习是在交互中产生样本的，是一个产生样本、算法更新、再次产生样本、再次算法更新的动态循环训练过程，而不是一个准备样本、算法更新的静态训练过程。

$\qquad$ 这本质上还是跟要解决的问题不同有关，强化学习解决的是序列决策问题，而深度学习解决的是“打标签”问题，即给定一张图片，我们需要判断这张图片是猫还是狗，这里的猫和狗就是标签，当然也可以让算法自动打标签，这就是监督学习与无监督学习的区别。而强化学习解决的是“打分数”问题，即给定一个状态，我们需要判断这个状态是好还是坏，这里的好和坏就是分数。当然，这只是一个比喻，实际上强化学习也可以解决“打标签”问题，只不过这个标签是一个连续的值，而不是离散的值，比如我们可以给定一张图片，然后判断这张图片的美观程度，这里的美观程度就是一个连续的值，而不是离散的值。

$\qquad$ 如图 $\text{6-2}$ 所示，除了训练生成模型之外，强化学习相当于在深度学习的基础上增加了一条回路，即继续与环境交互产生样本。相信学过控制系统的读者很快会意识到，这个回路就是一个典型的反馈系统机制，模型的输出一开始并不能达到预期的值，因此通过动态地不断与环境交互来产生一些反馈信息，从而训练出一个更好的模型。

<div align=center>
<img width="600" src="../figs/ch6/dl_rl.png"/>
</div>
<div align=center>图 $\text{6-2}$ 深度学习与强化学习示例</div>



## 线性回归

$\qquad$ 本小节开始总结归纳强化学习用到的一些深度学习模型，首先是线性模型。严格来说，线性模型并不是深度学习模型，而是传统的机器学习模型，但它是深度学习模型的基础，在深度学习中相当于单层的神经网络。在线性模型中，应用较为广泛的两个基础模型就是线性回归和逻辑回归，通常分别用于解决回归和分类问题，尽管后者也可以用来解决回归问题。

$\qquad$ 以典型的房价预测问题为例，假设一套房子有 $m$ 个特征，例如建造年份、房子面积等，分别记为 $x_1, x_2, \cdots, x_m$，用向量表示为 $\text{(4.1)}$ 。

$$
\tag{4.1}
\boldsymbol{x}=\left[x_1, x_2, \cdots, x_m\right]
$$

$\qquad$ 那么房价 $y$ 可以表示为式 $\text{(4.2)}$。

$$
\tag{4.2}
f(\boldsymbol{x} ; \boldsymbol{w}, b) = w_1 x_1+w_2 x_2+\cdots+w_m x_m+b = \boldsymbol{w}^T \boldsymbol{x}+b
$$

$\qquad$ 其中 $\boldsymbol{w}$ 和 $b$ 是模型的参数，$f(\boldsymbol{x} ; \boldsymbol{w}, b)$ 是模型的输出，也就是我们要预测的房价。出于简化考虑，通常我们会用一个符号 $\boldsymbol{\theta}$ 来表示 $\boldsymbol{w}$ 和 $b$，如式 $\text{(4.3)}$ 所示。

$$
\tag{4.3}
f^{\theta}(\boldsymbol{x}) = \boldsymbol{\theta}^T \boldsymbol{x}
$$

$\qquad$ 在这类问题中，这样的关系可以用模型来表述，我们的目标是求得一组最优的参数 $\boldsymbol{\theta^{*}}$ ，使得该模型尽可能地能够根据房屋的 $m$ 个特征准确预测对应的房价。这类问题也叫做拟合问题，并且注意是近似求解，因为几乎不可能找到一种模型能够完美拟合所有的样本，即找到最优解，甚至这个最优解不一定存在。因此，这类问题也普遍存在过拟合和欠拟合的问题，过拟合是指在训练集上表现很好，但在测试集上表现很差，而欠拟合则是指在训练集上表现很差，但在测试集上表现很好。这两种情况都是不理想的，本质上都是陷入了局部最优解的问题，因此我们有时候需要一些方法来解决这个问题，比如正则化、数据增强等。

## 梯度下降

$\qquad$ 回到问题本身，这类问题的解决方法也有很多种，例如最小二乘法、牛顿法等，但目前最流行的方法还是梯度下降。其基本思想如下：

* 初始化参数：选择一个初始点或参数的初始值。
* 计算梯度：在当前点计算函数的梯度，即函数关于各参数的偏导数。梯度指向函数值增加最快的方向。
* 更新参数：按照负梯度方向更新参数，这样可以减少函数值。这个过程在神经网络中一般是以反向传播算法来实现的。
* 重复上述两个步骤，直到梯度趋近于 0 或者达到一定迭代次数。

$\qquad$ 梯度下降本质上是一种基于贪心思想的方法,它的泛化能力很强，能够基于任何**可导的函数**求解最优解。。如图 $\text{6-3}$ 所示，假设我们要找到一个山谷中的最低点，也就是下山，那么我们可以从任意一点出发，然后沿着最陡峭的方向向下走，这样就能够找到山谷中的最低点。这里的最陡峭的方向就是梯度方向，而沿着这个方向走的步长就是学习率，这个学习率一般是一个超参数，需要我们自己来设定。

$\qquad$ 一般情况下，我们会将学习率设定为一个较小的值，这样可以保证我们不会错过最低点，但是如果学习率过小，那么我们就需要更多的迭代次数才能够达到最低点。每次梯度下降的迭代过程中，我们都会选取一个小批量的样本来计算梯度，这个小批量的样本称为一个 $\text{batch}$ 。这个批量相当于下山过程中我们看到的视野，如果批量太小的话，由于看不到更远的地方，我们就很容易被一些局部的山峰所迷惑，即陷入局部最优解。但是如果批量太大的话，那么我们就需要更多的计算资源。因此，我们需要根据实际情况来选择一个合适的 $\text{batch}$ 大小。

<div align=center>
<img width="600" src="../figs/ch6/gd.png"/>
</div>
<div align=center>图 $\text{6-3}$ 梯度下降示例</div>

$\qquad$ 除了调整学习率和批量大小之外，我们还可以对梯度下降的机制进行一些处理，比如加入动量、$\text{Adam}$ 等，这类工具我们一般称之为优化器（ $\text{optimizer}$ ）。动量法的基本思想是在梯度下降的过程中，不仅仅考虑当前的梯度，还要考虑之前的梯度，这样可以加快梯度下降的速度，同时也可以减少梯度下降过程中的震荡。$\text{Adam}$  是一种自适应的优化算法，它不仅仅考虑了当前的梯度，还考虑了之前的梯度的平方，这样可以更加准确地估计梯度的方向，从而加快梯度下降的速度，也是目前最流行的优化器之一。注意在做强化学习应用或研究的时候，我们并不需要太纠结于优化器的选择，因为这些优化器的效果并没有太大的差别，而且我们也不需要去了解它们的具体原理，只需要知道它们的大致作用就可以了。

## 逻辑回归

$\qquad$ 简单介绍完梯度下降之后，我们就可以继续介绍一些模型了，现在是逻辑回归。注意，虽然逻辑回归名字中带有回归，但是它是用来解决分类问题的，而不是回归问题（即预测问题）。在分类问题中，我们的目标是预测样本的类别，而不是预测一个连续的值。例如，我们要预测一封邮件是否是垃圾邮件，这就是一个二分类问题，通常输出 $0$ 和 $1$ 等离散的数字来表示对应的类别。在形式上，逻辑回归和线性回归非常相似，如图 $\text{6-4}$ 所示，就是在线性模型的后面增加一个 $\text{sigmoid}$ 函数，我们一般称之为激活函数。

<div align=center>
<img width="800" src="../figs/ch6/logistic_struction.png"/>
</div>
<div align=center>图 $\text{6-4}$ 逻辑回归结构</div>

$\qquad$ $\text{sigmoid}$ 函数定义为：

$$
sigmoid(z) = \frac{1}{1+exp(-z)}
$$

$\qquad$ 如图 $\text{6-5}$ 所示，$\text{sigmoid}$ 函数可以将输入的任意实数映射到 $0-1$ 之间，对其输出的值进行判断，例如小于 $0.5$ 我们认为预测的是类别 $0$，反之是类别 $1$ ，这样一来通过梯度下降来求解模型参数就可以用于实现二分类问题了。注意，虽然逻辑回归只是在线性回归模型基础上增加了一个激活函数，但两个模型是完全不同的，包括损失函数等等。线性回归的损失函数是均方差损失，而逻辑回归模型一般是交叉熵损失，这两种损失函数在深度学习和深度强化学习中都很常见。

<div align=center>
<img width="400" src="../figs/ch6/sigmoid.png"/>
</div>
<div align=center>图 $\text{6-5}$ $\text{sigmoid}$ 函数图像</div>

$\qquad$ 逻辑回归的主要优点在于增加了模型的非线性能力，同时模型的参数也比较容易求解，但是它也有一些缺点，例如它的非线性能力还是比较弱的，而且它只能解决二分类问题，不能解决多分类问题。在实际应用中，我们一般会将多个二分类问题组合成一个多分类问题，例如将 $\text{sigmoid}$ 函数 换成 $\text{softmax}$ 回归函数等。

$\qquad$ 同时，逻辑回归的模型结构也比较灵活多变

## 神经网络

### 全连接网络

全连接网络又称作**多层感知机（multi-layer perceptron，MLP）**，是最基础的神经网络模型。它是基于生物神经网络的启发，将“线性函数+激活函数”这样的结构一层层堆叠（stack）组合成一个多层的网络模型，用于解决更复杂的问题。如\figref{fig:ann_vs_dnn}所示，线性函数可以看做生物神经网络的神经元，而激活函数就是神经元之间的突触结构。顺便说一句，生物神经网络是神奇且复杂的，人们也一直在尝试研究新的人工神经网络模型去模拟生物神经网络，例如脉冲神经网络，尽管目前这些模型还没有得到广泛地验证。

<div align=center>
<img width="800" src="../figs/ch6/ann_vs_dnn.png"/>
</div>
<div align=center>图 6.3 生物神经网络与人工神经网络的对比</div>

记神经网络模型中上一层的输入向量为$\boldsymbol{x^{l-1}}\in \mathbb{R}^{d^{l-1}}$，其中第一层的输入也就是整个模型的输入可记为$\boldsymbol{x^0}$，每一个全连接层将上一层的输入映射到$\boldsymbol{x^{l}}\in \mathbb{R}^{d^{l}}$，也就是下一层的输入，具体定义为：

$$
\boldsymbol{x}^{l}=\sigma(\boldsymbol{z}), \quad \boldsymbol{z}=\boldsymbol{W} \boldsymbol{x^{l-1}}+\boldsymbol{b} = \boldsymbol{\theta} \boldsymbol{x^{l-1}}
$$

其中$\boldsymbol{W}\in \mathbb{R}^{d^{l-1} \times d^{l}}$是权重矩阵，$\boldsymbol{b}$为偏置矩阵，与线性模型类型，这两个参数我们通常看作一个参数$\boldsymbol{\theta}$。$\sigma(\cdot)$是激活函数，除了 Sigmoid 函数之外，还包括 Softmax 函数、ReLU 函数和 tanh 函数等等激活函数。其中最常用的是 ReLU 函数 和 tanh 函数，前者将神经元也就是线性函数的输出映射到$0-1$之间，后者则映射到$-1$到$1$之间。前面讲到，在强化学习中我们用神经网络来近似动作价值函数，动作价值函数的输入是状态，输出是各个动作对应的价值，在有些连续动作问题中比如汽车方向盘转动角度是$-90$度到$90$度之间，这种情况下使用 tanh 激活函数能够使得神经网络负值以便于更好地近似状态动作函数。顺便提一句，这里还有一种做法是我们可以把动作空间映射到正值的范围，例如$0$到$180$区间，这样一来对应的神经网络模型激活函数使用 ReLU 函数会更好些。

一个$l$层的神经网络模型可以表示为：
$$
\begin{split}
    第 1 层: \quad \boldsymbol{x}^{(1)}=\sigma_1\left(\boldsymbol{W}^{(1)} \boldsymbol{x}^{(0)}+\boldsymbol{b}^{(1)}\right),\\
    第 2 层: \quad \boldsymbol{x}^{(2)}=\sigma_2\left(\boldsymbol{W}^{(2)} \boldsymbol{x}^{(1)}+\boldsymbol{b}^{(2)}\right),\\
    \vdots \quad \vdots\\
    第 l 层: \quad \boldsymbol{x}^{(l)}=\sigma_l\left(\boldsymbol{W}^{(l)} \boldsymbol{x}^{(l-1)}+\boldsymbol{b}^{(l)}\right)\\
\end{split}
$$

求解神经网络模型参数的方法除了梯度下降之外，还涉及多层网络模型的正向传播和反向传播，具体细节在接下来的小节中展开。

### 卷积神经网络

### 循环神经网络

### Transformer

## 梯度下降

## 反向传播
