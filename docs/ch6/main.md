# 深度学习基础

$\qquad$ 在前面章节中我们主要介绍了传统强化学习的内容，这些内容涵盖了基础的问题核心和解决方法。但是对应的算法并不能解决高维度的复杂问题，因此现在普遍流行将深度学习和强化学习结合起来，利用深度学习网络强大的拟合能力通过将状态、动作等作为输入，来估计对应的状态价值和动作价值等等。

前面章节中都属于基础的强化学习内容，在后面的章节中我们主要讲解深度强化学习，在此之前会为读者介绍强化学习中涉及到的一些深度学习基础，以便有一个过渡。

## 线性模型

首先介绍线性模型，线性模型是最简单的一类机器学习模型，可以将其视为单层的神经网络。在线性模型中最基础的两个模型就是线性回归和逻辑回归，通常分别用于解决回归和分类问题，尽管后者也可以用来解决回归问题。回归模型的输出是一个连续的值，而分类模型的输出是离散的值，用于表示分类。本质上来说回归模型和分类模型是一样的，分类模型可以将回归模型离散化，例如本节将要讲的逻辑回归就是在线性回归的基础上增加了一个 sigmoid 函数对其进行了离散化。顺便提一句，回归模型也可以将分类模型连续化，通常见于贝叶斯模型中，但这在强化学习中并不常用。

**线性回归**。以Kaggle入门竞赛项目房价预测为例，一套房子有$m$个特征，例如建造年份、房子面积等等，把这$m$个特征用向量表示，如下：

$$
\boldsymbol{x}=\left[x_1, x_2, \cdots, x_m\right]
$$

我们可以用线性模型来拟合这$m$个特征和房价的关系，如下：

$$
f(\boldsymbol{x} ; \boldsymbol{w}, b) = w_1 x_1+w_2 x_2+\cdots+w_m x_m+b = \boldsymbol{w}^T \boldsymbol{x}+b
$$

其中$\boldsymbol{w}$和$b$是模型的参数，$f(\boldsymbol{x} ; \boldsymbol{w}, b)$是模型的输出，也就是我们要预测的房价。出于简化考虑，通常我们会用一个符号$\boldsymbol{\theta}$来表示$\boldsymbol{w}$和$b$，如下：

$$
f^{\theta}(\boldsymbol{x}) = \boldsymbol{\theta}^T \boldsymbol{x}
$$

我们的目的是求得一组最优的参数$\boldsymbol{\theta^{*}}$，使得该模型能够根据房屋的$m$个特征预测对应的房价。我们一般利用历史数据来近似求解最优参数，这个过程就叫做训练。注意这里是近似求解，因为几乎所有机器学习模型都无法找到一种方法能够获得绝对的最优解，甚至也不一定存在绝对最优解，有些方法甚至很容易陷入局部最小值的问题中。训练的方法，或者说求解模型参数的方法理论上来说有很多种，比如这里线性模型可以用最小二乘法来求解，另外有些模型可以用牛顿法来求解，而目前普遍流行的优化方法就是梯度下降。梯度下降方法泛化能力很强，能够基于梯度下降求解很多种模型，该方法的本质是一阶泰勒展开，顺便提一句，牛顿法则是二阶泰勒展开。

**逻辑回归**。对于分类问题，其预测目标不再是连续的值，而可能是二元变量，要么等于0，要么等于1，即最简单的二分类问题。这种情况下，我们可以用逻辑回归来解决，注意虽然逻辑回归名字里带有回归，但通常用于解决二分类问题而非回归问题。逻辑回归的思路也比较简单，如图 6.1 所示，就是在线性模型的后面增加一个sigmoid函数，我们一般称之为激活函数。逻辑回归模型其实可以看做神经网络模型的一个神经元，具体后面再展开说明。

<div align=center>
<img width="800" src="../figs/ch6/logistic_struction.png"/>
</div>
<div align=center>图 6.1 逻辑回归结构</div>

sigmoid 函数定义为：

$$
sigmoid(z) = \frac{1}{1+exp(-z)}
$$

如图 6.2 所示，sigmoid 函数可以将输入的任意实数映射到$0-1$之间，对其输出的值进行判断，例如小于0.5我们认为预测的是类别0，反之是类别1，这样一来通过梯度下降来求解模型参数就可以用于实现二分类问题了。注意，虽然逻辑回归只是在线性回归模型基础上增加了一个激活函数，但两个模型是完全不同的，包括损失函数等等。线性回归的损失函数是均方差损失，而逻辑回归模型一般是交叉熵损失，这两种损失函数在深度学习和深度强化学习中都很常见，具体推导细节感兴趣的读者可自行翻阅相关资料。

<div align=center>
<img width="400" src="../figs/ch6/sigmoid.png"/>
</div>
<div align=center>图 6.2 Sigmoid函数图像</div>

## 神经网络

### 全连接网络

全连接网络又称作**多层感知机（multi-layer perceptron，MLP）**，是最基础的神经网络模型。它是基于生物神经网络的启发，将“线性函数+激活函数”这样的结构一层层堆叠（stack）组合成一个多层的网络模型，用于解决更复杂的问题。如\figref{fig:ann_vs_dnn}所示，线性函数可以看做生物神经网络的神经元，而激活函数就是神经元之间的突触结构。顺便说一句，生物神经网络是神奇且复杂的，人们也一直在尝试研究新的人工神经网络模型去模拟生物神经网络，例如脉冲神经网络，尽管目前这些模型还没有得到广泛地验证。

<div align=center>
<img width="800" src="../figs/ch6/ann_vs_dnn.png"/>
</div>
<div align=center>图 6.3 生物神经网络与人工神经网络的对比</div>

记神经网络模型中上一层的输入向量为$\boldsymbol{x^{l-1}}\in \mathbb{R}^{d^{l-1}}$，其中第一层的输入也就是整个模型的输入可记为$\boldsymbol{x^0}$，每一个全连接层将上一层的输入映射到$\boldsymbol{x^{l}}\in \mathbb{R}^{d^{l}}$，也就是下一层的输入，具体定义为：

$$
\boldsymbol{x}^{l}=\sigma(\boldsymbol{z}), \quad \boldsymbol{z}=\boldsymbol{W} \boldsymbol{x^{l-1}}+\boldsymbol{b} = \boldsymbol{\theta} \boldsymbol{x^{l-1}}
$$

其中$\boldsymbol{W}\in \mathbb{R}^{d^{l-1} \times d^{l}}$是权重矩阵，$\boldsymbol{b}$为偏置矩阵，与线性模型类型，这两个参数我们通常看作一个参数$\boldsymbol{\theta}$。$\sigma(\cdot)$是激活函数，除了 Sigmoid 函数之外，还包括 Softmax 函数、ReLU 函数和 tanh 函数等等激活函数。其中最常用的是 ReLU 函数 和 tanh 函数，前者将神经元也就是线性函数的输出映射到$0-1$之间，后者则映射到$-1$到$1$之间。前面讲到，在强化学习中我们用神经网络来近似动作价值函数，动作价值函数的输入是状态，输出是各个动作对应的价值，在有些连续动作问题中比如汽车方向盘转动角度是$-90$度到$90$度之间，这种情况下使用 tanh 激活函数能够使得神经网络负值以便于更好地近似状态动作函数。顺便提一句，这里还有一种做法是我们可以把动作空间映射到正值的范围，例如$0$到$180$区间，这样一来对应的神经网络模型激活函数使用 ReLU 函数会更好些。

一个$l$层的神经网络模型可以表示为：
$$
\begin{split}
    第 1 层: \quad \boldsymbol{x}^{(1)}=\sigma_1\left(\boldsymbol{W}^{(1)} \boldsymbol{x}^{(0)}+\boldsymbol{b}^{(1)}\right),\\
    第 2 层: \quad \boldsymbol{x}^{(2)}=\sigma_2\left(\boldsymbol{W}^{(2)} \boldsymbol{x}^{(1)}+\boldsymbol{b}^{(2)}\right),\\
    \vdots \quad \vdots\\
    第 l 层: \quad \boldsymbol{x}^{(l)}=\sigma_l\left(\boldsymbol{W}^{(l)} \boldsymbol{x}^{(l-1)}+\boldsymbol{b}^{(l)}\right)\\
\end{split}
$$

求解神经网络模型参数的方法除了梯度下降之外，还涉及多层网络模型的正向传播和反向传播，具体细节在接下来的小节中展开。

### 卷积神经网络

### 循环神经网络

### Transformer

## 梯度下降

## 反向传播
