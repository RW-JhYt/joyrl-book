# 策略梯度

本章开始介绍基于策略梯度（policy based）的算法，与前面介绍的基于价值（value based）的算法（包括Q learning，SARSA以及DQN等等）不同，这类算法直接对策略本身进行近似优化。在这种情况下，我们可以将策略描述成一个带有参数$\theta$的连续函数，该函数将某个状态作为输入，输出的不再是某个确定性（deterministic）的离散动作，而是对应的动作概率分布，通常用$\pi_{\theta}(a|s)$表示，称作随机性（stochastic）策略。下面我们将从最基本的策略梯度算法展开。

## 策略梯度算法

尽管策略梯度算法是将策略参数化成一个连续的函数$\pi_{\theta}(a|s)$，但是与基于价值的算法本质上是一样的，最终的优化目标都是累积的价值期望$V^{*}(s)$。例如在前面章节的 Q learning 算法中，我们利用贝尔曼方程求解马尔科夫决策过程中的最佳决策序列，进而求出对应的最优动作价值函数$Q^{*}(s,a)$，不清楚的读者可以再回到前面的内容温习一下。

策略一般记作 $\pi$。假设我们使用深度学习来做强化学习，策略就是一个网络。网络里面有一些参数，我们用 $\theta$ 来代表 $\pi$ 的参数。
网络的输入是智能体看到的东西，如果让智能体玩视频游戏，智能体看到的东西就是游戏的画面。智能体看到的东西会影响我们训练的效果。例如，在玩游戏的时候， 也许我们觉得游戏的画面是前后相关的，所以应该让策略去看从游戏开始到当前这个时间点之间所有画面的总和。因此我们可能会觉得要用到循环神经网络（recurrent neural network，RNN）来处理它，不过这样会比较难处理。
我们可以用向量或矩阵来表示智能体的观测，并将观测输入策略网络，策略网络就会输出智能体要采取的动作。
\figref{fig:actor_policy} 就是具体的例子，策略是一个网络；输入是游戏的画面，它通常是由像素组成的；输出是我们可以执行的动作，有几个动作，输出层就有几个神经元。假设我们现在可以执行的动作有 3 个，输出层就有 3 个神经元，每个神经元对应一个可以采取的动作。输入一个东西后，网络会给每一个可以采取的动作一个分数。我们可以把这个分数当作概率，演员根据概率的分布来决定它要采取的动作，比如 0.7 的概率向左走、0.2 的概率向右走、0.1的概率开火等。概率分布不同，演员采取的动作就会不一样。

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.5\linewidth]{ch6/figs/actor_policy.png}
    \caption{演员的策略}
    \label{fig:actor_policy}
\end{figure}

接下来我们用一个例子来说明演员与环境交互的过程。