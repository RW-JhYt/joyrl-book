# 绪论

## 简介

你好，亲爱的读者！无论你是出于何种原因选择了本书，本书的笔者都将不胜感激，并且尽可能地带你走进一个更接地气的强化学习新世界。在生活中我们常常有很多种选择，每一种选择就是一次**决策**，每一次决策都会带来相应的后果，这个后果可能是好的，也可能是坏的，也可能是即时的，比如我们吃到棉花糖就能立刻感受到它的甜度，也可能是延时的，比如寒窗十年方得一日踏阅长安花。我们也会及时反思以往作出的每一次决策，这其中可能有懊悔，也可能有庆幸，每一次反思总结都能增长我们的经验，并激励着我们在下一次的决策中做出更加明智的选择。人生如此，在机器学习的某个子领域中也是如此，我们称之为序列决策（sequential decision making）。而目前解决序列决策问题最为行之有效的方法就是**强化学习（reinforcement learning，RL）**，即本书的主题。

关于本书的初衷。其实日前强化学习相关的书籍在市面上已经琳琅满目了，但是这些普遍偏向理论，缺少一些实际的经验性总结，比如大佬们可能会通过数学推导来告诉你某某算法是可行的，可是一些实验细节和不同算法的对比很难在这些书籍中体现出来，理论与实践之间、公式与代码之间其实存在着一定的鸿沟。另一方面，由于信息时代知识的高速迭代，面对如海洋一般的信息，我们需要从中梳理出重点并快速学习，以便于尽快看到实际应用的效果，而这中间就不得不需要一个经验丰富的老师傅来带路了，这也是本书的初衷之一。笔者会基于大量的强化学习实践经验，对于理论部分删繁就简，并与实践紧密结合，以更通俗易懂的方式帮助读者们快速实践。
## 强化学习概述

强化学习是广义机器学习的一个领域，注意这里广义机器学习泛指涵盖几乎所有人工智能方法的领域，包括狭义的机器学习（例如线性模型，支持向量机等）和深度学习等等。从实现的角度来讲，这三类其实都是基于 **大量的样本** 来对相应算法进行迭代更新并且达到最优的，我们称之为**训练**。但与另外两者不同的是，强化学习是在交互中产生样本的，是一个产生样本、算法更新、再次产生样本、再次算法更新的动态循环训练过程，而不是一个准备样本、算法更新的静态训练过程，这本质上还是跟要解决的问题不同有关，强化学习解决的是序列决策问题，而深度学习解决的是“打标签”问题，即给定一张图片，我们需要判断这张图片是猫还是狗，这里的猫和狗就是标签，当然也可以让算法自动打标签，这就是监督学习与无监督学习的区别。而强化学习解决的是“打分数”问题，即给定一个状态，我们需要判断这个状态是好还是坏，这里的好和坏就是分数。当然，这只是一个比喻，实际上强化学习也可以解决“打标签”问题，只不过这个标签是一个连续的值，而不是离散的值，比如我们可以给定一张图片，然后判断这张图片的美观程度，这里的美观程度就是一个连续的值，而不是离散的值。强化学习的应用场景非常广泛，比如机器人、自动驾驶、游戏、金融等等，这些领域都是需要在不断的交互中产生样本并且进行算法更新的，而不是一次性产生样本并且进行算法更新的。

严格来讲，强化学习隶属于动态系统理论的范畴，涉及马尔可夫决策问题（序列决策问题的数学描述）的最优化控制（感兴趣的读者可以深入了解一下相关知识），这点在后面马尔可夫过程相关章节中会详细展开。在这个最优化控制过程中，智能体必须能够感知到环境的状态并采取合适的动作，对应的动作又会影响到环境的状态，而能够解决这一类问题的方法我们都统称为强化学习方法。
