# 绪论

$\qquad$ 在正式介绍具体的强化学习算法之前，本章将宏观地讨论一下强化学习（ $\text{reinforcement learning，RL}$ ）的相关概念以及应用等，帮助读者更好地 “观其大略” 。尤其是对于想用强化学习做一些交叉研究的读者来说，更应该首先通过本章了解强化学习是什么，大概能做什么，能做到什么样的效果等等，而不是直接从一个个的算法开始。强化学习发展至今，尽管算法已经有成百上千种样式，但实际上从大类来看要掌握的核心并不多，大多数算法都只是做了一些较小的改进。举个例子，如图 $\text{1.1}$ 所示，我们知道用水加上咖啡豆通过一定的方法就能调制成咖啡，加上糖块就能变成糖水，虽然看起来形式不同，但本质上都是水，只是为了符合不同的口味而已。

<div align=center>
<img width="400" src="../figs/ch1/water_examp.png"/>
</div>
<div align=center>图 $\text{1.1}$ 咖啡与糖水的示例</div>

## 为什么选择强化学习？

$\qquad$ 首先，我们先讨论一下为什么要学习强化学习，强化学习对于我们的意义是什么。可能大部分读者都是通过先了解到人工智能和机器学习才了解到强化学习的，但实际上早在我们认知人工智能之前就已经不知不觉地接触到了强化学习。笔者想起了初中生物课本中关于蚯蚓的一个实验，其内容大致是这样的：如图 $\text{1.2}$ 所示，将蚯蚓放在一个盒子中，盒子中间有一条分叉路口，路口尽头分别放有食物和电极，让蚯蚓自己爬行到其中一个路口的尽头，在食物的一端蚯蚓会品尝到美味，而在电极的一头则会受到轻微的电击。

<div align=center>
<img width="200" src="../figs/ch1/qiuyin.png"/>
</div>
<div align=center>图 $\text{1.2}$ 蚯蚓实验</div>

$\qquad$ 实验的目的是希望蚯蚓能一直朝着有食物的一段路口爬行，但由于蚯蚓没有视力，因此一开始可能蚯蚓会一直朝着有电极的路口爬行并最后遭到电击。每次蚯蚓遭受到电击或者吃到食物之后会从头开始，经过多次实验，蚯蚓会逐渐学会朝着有食物的路口爬行，而不是朝着有电极的路口爬行。在这个过程中，蚯蚓是在不断地尝试并试错中学习到了正确的策略，尽管在中学生物课本中这个实验的目的是为了说明蚯蚓的运动是由外界刺激所驱动的，而不是蚯蚓自身的意志所驱动的。但在今天，从人工智能的角度来看，这其实带着较为鲜明的强化学习的味道，即 “试错学习”（ $\text{try and error learning}$ ）。

$\qquad$ 试错学习一开始是和行为心理学等工作联系在一起的，主要包括以下几个关键部分：

* 尝试：即采取一系列动作或行为来尝试解决问题或实现目标。
* 错误：在尝试的过程中可能会出现错误，这些错误可能是由于环境的不确定性导致的，也可能是由于自身的不当行为导致的。
* 结果：每次尝试的后果，无论是积极的还是消极的，都会对下一次尝试产生影响。
* 学习：通过不断地尝试和错误，自身会逐渐积累经验，了解哪些行为会带来有利的结果，从而在下一次尝试中做出更加明智的选择。

试错学习的过程在我们日常生活中屡见不鲜，并且通常与其他形式的学习形成对比，例如经典条件反射（巴甫洛夫条件反射）和观察学习（通过观察他人来学习）。注意，尽管是强化学习中最鲜明的要素之一，但试错学习并不是强化学习的全部，甚至还会包含其它的学习形式例如观察学习（对应模仿学习、离线强化学习等技术）。

$\qquad$ 从另一方面来讲，在学习过程中个人做出的每一次尝试就是一种**决策** （ $\text{decision}$ ），每一次决策都会带来相应的后果，这个后果可能是好的，也可能是坏的，也可能是即时的，比如我们吃到棉花糖就能立刻感受到它的甜度，也可能是延时的，比如寒窗苦读十年之后，方得一日踏阅长安花。我们把好的结果称之为奖励（ $\text{reward}$ ），坏的结果称为惩罚（ $\text{punishment}$ ）或者负的奖励。最终通过一次次的决策来实现目标，这个目标通常是以最大化累积的奖励来呈现的，这个过程就是
**序列决策过程**（ $\text{sequential decision making}$ ），而强化学习就是解决序列决策问题的有效方法之一，即本书的主题。换句话说，对于任意问题，只要能够建模成序列决策问题或者带有鲜明的试错学习特征，那么就可以使用强化学习来解决，并且是截至目前最为高效的方法之一，这就是为什么要学习强化学习的原因。

## 强化学习的应用

$\qquad$ 前面小节中我们了解了强化学习大概是用来做什么的，那么它能做到什么样的效果呢？本节我们就来看看强化学习的一些例子和实际应用。强化学习的应用场景非常广泛，其中最为典型的场景就是游戏，以 $\text{AlphaGo}$ 为代表的围棋 $\text{AI}$ 就是强化学习的代表作之一，也是其为人们广泛熟知的得意之作。不光是各种棋类游戏，以 $\text{AlphaStar}$ 为代表的星际争霸 $\text{AI}$，以 $\text{AlphaZero}$ 为代表的通用游戏 $\text{AI}$，以及近年的 $\text{OpenAI Five}$ 为代表的 $\text{Dota2 AI}$，这些都是强化学习在游戏领域的典型应用。

$\qquad$ 除了游戏领域之外，强化学习在机器人领域（ $\text{robot manipulation}$ ）中也有所应用。举个例子，图 $\text{1.3}$ <sup>①</sup> 演示了 $\text{NICO}$ 机器人在学习抓取任务的过程。该任务的目标是将桌面上的物体抓取到指定的位置，机器人通过每次输出相应关节的参数来活动手臂，然后通过摄像头来观测当前的状态，最后通过人为设置的奖励（例如接近目标就给一个正向的奖励）来学习到正确的抓取策略。

<div align=center>
<img width="200" src="../figs/ch1/NICO_grasping.png"/>
</div>
<div align=center>图 $\text{1.3}$ $\text{NICO}$ 机器人抓取任务</div>

> ① 图片来源：https://www2.informatik.uni-hamburg.de/WTM/projects/NeuralGrasp/NeuralGrasp.shtml

$\qquad$ 不同于游戏，在机器人中实现强化学习的成本往往较为高昂，一方面观测环境的状态需要大量的传感器，另一方面则是试错学习带来的实验成本，在训练过程中如果机器人决策稍有失误就有可能导致设备损坏，因此在实际应用中往往需要结合其他的方法来辅助强化学习进行决策。其中最典型的做法就是建立一个仿真环境，通过仿真环境来模拟真实环境，这样就可以大大降低实验成本。如图 $\text{1.4}$ 所示 <sup>②</sup>，该环境模拟了真实的机械臂抓取任务，通过仿真环境免去大量视觉传感器的搭建过程从而可以大大降低实验成本，同时由于仿真环境中机器人关节响应速度更快进而加快算法的迭代速度，从而更快地得到一个较好的策略。

<div align=center>
<img width="200" src="../figs/ch1/gym_robot.png"/>
</div>
<div align=center>图 $\text{1.4}$ 机器人抓取任务仿真</div>

> ② 图片来源：https://gym.openai.com/envs/#robotics

$\qquad$ 当然，仿真环境也并不是万能的，因为仿真环境和真实环境之间往往存在着一定的差异，这就需要我们在设计仿真环境的时候尽可能地考虑到真实环境的各种因素，这也是一个非常重要的研究方向。除了最简单的抓取任务之外，研究者们还在探索将强化学习应用于更加复杂的机器人任务，例如仓储搬运、机器人足球以及自动驾驶等等。

$\qquad$ 除了游戏和机器人领域之外，强化学习在金融领域也有所应用，例如股票交易、期货交易、外汇交易等等。举个例子，图 $\text{1.5}$ <sup>③</sup> 演示了一个股票交易的例子，我们的目标是通过买卖股票来最大化我们的资产。在这个过程中，我们需要不断地观测当前的股票价格，然后根据当前的价格来决定买入或卖出股票的数量，最后通过股票价格的变化来更新我们的资产。在这个过程中，我们的资产会随着股票价格的变化而变化，这就是奖励，我们的目标是通过不断地买卖股票来最大化我们的资产，这就是序列决策过程，而强化学习就是解决序列决策问题的有效方法之一。



## 强化学习方向概述
## 学习强化学习之前的一些准备

关于本书的初衷。其实日前强化学习相关的书籍在市面上已经琳琅满目了，但是这些普遍偏向理论，缺少一些实际的经验性总结，比如大佬们可能会通过数学推导来告诉你某某算法是可行的，可是一些实验细节和不同算法的对比很难在这些书籍中体现出来，理论与实践之间、公式与代码之间其实存在着一定的鸿沟。另一方面，由于信息时代知识的高速迭代，面对如海洋一般的信息，我们需要从中梳理出重点并快速学习，以便于尽快看到实际应用的效果，而这中间就不得不需要一个经验丰富的老师傅来带路了，这也是本书的初衷之一。笔者会基于大量的强化学习实践经验，对于理论部分删繁就简，并与实践紧密结合，以更通俗易懂的方式帮助读者们快速实践。
## 强化学习概述

强化学习是广义机器学习的一个领域，注意这里广义机器学习泛指涵盖几乎所有人工智能方法的领域，包括狭义的机器学习（例如线性模型，支持向量机等）和深度学习等等。从实现的角度来讲，这三类其实都是基于 **大量的样本** 来对相应算法进行迭代更新并且达到最优的，我们称之为**训练**。但与另外两者不同的是，强化学习是在交互中产生样本的，是一个产生样本、算法更新、再次产生样本、再次算法更新的动态循环训练过程，而不是一个准备样本、算法更新的静态训练过程，这本质上还是跟要解决的问题不同有关，强化学习解决的是序列决策问题，而深度学习解决的是“打标签”问题，即给定一张图片，我们需要判断这张图片是猫还是狗，这里的猫和狗就是标签，当然也可以让算法自动打标签，这就是监督学习与无监督学习的区别。而强化学习解决的是“打分数”问题，即给定一个状态，我们需要判断这个状态是好还是坏，这里的好和坏就是分数。当然，这只是一个比喻，实际上强化学习也可以解决“打标签”问题，只不过这个标签是一个连续的值，而不是离散的值，比如我们可以给定一张图片，然后判断这张图片的美观程度，这里的美观程度就是一个连续的值，而不是离散的值。强化学习的应用场景非常广泛，比如机器人、自动驾驶、游戏、金融等等，这些领域都是需要在不断的交互中产生样本并且进行算法更新的，而不是一次性产生样本并且进行算法更新的。

严格来讲，强化学习隶属于动态系统理论的范畴，涉及马尔可夫决策问题（序列决策问题的数学描述）的最优化控制（感兴趣的读者可以深入了解一下相关知识），这点在后面马尔可夫过程相关章节中会详细展开。在这个最优化控制过程中，智能体必须能够感知到环境的状态并采取合适的动作，对应的动作又会影响到环境的状态，而能够解决这一类问题的方法我们都统称为强化学习方法。
