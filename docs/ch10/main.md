# Actor-Critic 算法

在策略梯度的章节中，实际上我们已经开了一部分 `Actor-Critic` 算法的头了，这一章我们将继续深入探讨 `Actor-Critic` 算法。

## 策略梯度算法的缺点

这里策略梯度算法特指蒙特卡洛策略梯度算法，相比于 `DQN` 之类的基于价值的算法，策略梯度算法有以下优点：

* **适配连续动作空间**。在将策略函数设计的时候我们已经展开过，这里不再赘述。
* **适配随机策略**。由于策略梯度算法是基于策略函数的，因此可以适配随机策略，而基于价值的算法则需要一个确定的策略。此外其计算出来的策略梯度是无偏的，而基于价值的算法则是有偏的。

但同样的，策略梯度算法也有其缺点：

* **采样效率低**。由于使用的是蒙特卡洛估计，与基于价值算法的时序差分估计相比其采样速度必然是要慢很多的，这个问题在前面相关章节中也提到过。
* **高方差**。虽然跟基于价值的算法一样都会导致高方差，但是策略梯度算法通常是在估计梯度时蒙特卡洛采样引起的高方差，这样的方差甚至比基于价值的算法还要高。
* **收敛性差**。容易陷入局部最优，策略梯度方法并不保证全局最优解，因为它们可能会陷入局部最优点。策略空间可能非常复杂，存在多个局部最优点，因此算法可能会在局部最优点附近停滞。
* **难以处理高维离散动作空间**：对于离散动作空间，采样的效率可能会受到限制，因为对每个动作的采样都需要计算一次策略。当动作空间非常大时，这可能会导致计算成本的急剧增加。

而结合了策略梯度和值函数的 `Actor-Critic` 算法则能同时兼顾两者的优点，并且甚至能缓解两种方法都很难解决的高方差问题。可能读者会奇怪为什么各自都有高方差的问题，结合了之后反而缓解了这个问题呢？我们再仔细分析一下两者高方差的根本来源，策略梯度算法是因为直接对策略参数化，相当于既要利用策略去与环境交互采样，又要利用采样去估计策略梯度，而基于价值的算法也是需要与环境交互采样来估计值函数的，因此也会有高方差的问题。而结合之后呢，`Actor` 部分还是负责估计策略梯度和采样，但 `Critic` 即原来的值函数部分就不需要采样而只负责估计值函数了，并且由于它估计的值函数指的是策略函数的值，相当于带来了一个更稳定的估计，来指导 `Actor` 的更新，反而能够缓解策略梯度估计带来的方差。当然尽管 `Actor-Critic` 算法能够缓解方差问题，但并不能彻底解决问题，在接下来的章节中我们也会展开介绍一些改进的方法。


## Q Actor-Critic 算法

在策略梯度章节中，我们其实已经对 `Actor-Critic` 算法的目标函数进行过推导了，这里就不详细展开，只是简单回顾一下目标函数，如下：

$$
\begin{aligned}
\nabla_\theta J(\theta) \propto \mathbb{E}_{\pi_{\theta}}\left[Q^\pi(s, a) \nabla_\theta \log \pi_\theta(a \mid s)\right]
\end{aligned}
$$

在 `REINFORCE` 算法中，我们使用蒙特卡洛估计来表示当前状态-动作对 $(s_t,a_t)$ 的价值。而这里其实可以类比于 $Q$ 函数，用 $Q^\pi(s_t, a_t)$ 来估计当前的价值，注意这里的输入是状态和动作，而不单单是状态，输出的是单个值，也可以用 $Q_{\phi}(s_t, a_t)$ 表示，其中 $\phi$ 表示 Critic 网络的参数。这样我们就可以将目标函数写成：

$$
\begin{aligned}
\nabla_\theta J(\theta) \propto \mathbb{E}_{\pi_{\theta}}\left[Q_{\phi}(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t \mid s_t)\right]
\end{aligned}
$$

这样的算法通常称之为 `Q Actor-Critic` 算法，这也是最简单的 `Actor-Critic` 算法，现在我们一般都不用这个算法了，但是这个算法的思想是很重要的，因为后面的算法都是在这个算法的基础上进行改进的。

<div align=center>
<img width="300" src="../figs/ch10/actor_critic_architecture.png"/>
</div>
<div align=center>图 10.1 Actor-Critic 算法架构</div>

如图所示，我们通常将 `Actor` 和 `Critic` 分别用两个模块来表示，即图中的 策略函数（`Policy`）和价值函数（`Value Function`）。`Actor` 与环境交互采样，然后将采样的轨迹输入 `Critic` 网络，`Critic` 网络估计出当前状态-动作对的价值，然后再将这个价值作为 `Actor` 网络的梯度更新的依据，这也是所有 `Actor-Critic` 算法的基本通用架构。
## A2C 与 A3C 算法

我们知道 `Actor-Critic` 架构是能够缓解策略梯度算法的高方差问题的，但是并不能彻底解决问题。为了进一步缓解高方差问题，我们引入一个优势函数（Advantage Function）$A^\pi(s_t, a_t)$，用来表示当前状态-动作对相对于平均水平的优势，即：

$$
\begin{aligned}
A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)
\end{aligned}
$$

这里优势函数相当于减去了一个基线，这个基线可以自由设计，但是通常我们会选择状态价值函数 $V^\pi(s_t)$ 作为基线，减去这个基线会让梯度估计更稳定。有读者可能会奇怪，减去基线真的能减少方差吗？比如 $\{1,2,3\}$ 这个数列，都减去均值 $2$ 之后的数列 $\{-1,0,1\}$ 的方差不还是一样约等于 $0.67$ 吗? 这里其实犯了一个错误，就是我们讲的基线是指在同一个状态下的基线，而不是在整个数列上的均值，这里的基线是指 $V^\pi(s_t)$，而不是 $V^\pi(s)$ 的均值。

另一方面来讲，优势函数可以理解为在给定状态 $s_t$ 下，选择动作 $a_t$ 相对于平均水平的优势。如果优势为正，则说明选择这个动作比平均水平要好，反之如果为负则说明选择这个动作比平均水平要差。换句话说，原先对于每一个状态-动作对只能以自己为参照物估计，现在可以平均水平为参照物估计了，这样就能减少方差。这就好比我们练习马拉松，原先的做法是我们只关注于每天跑了多少，并不知道之前几天跑了多少，这很容易导致我们盲目追求每天跑动的距离，而忽略了自己的身体状况，导致受伤，也就得到了一个较差的跑步策略。而引入优势函数之后我们就可以知道之前几天的平均跑步距离，这样就能更好的控制自己的身体状况，避免受伤，并且更好地达到马拉松的目标。

有了优势函数之后，我们就可以将目标函数写成：

$$
\begin{aligned}
\nabla_\theta J(\theta) \propto \mathbb{E}_{\pi_{\theta}}\left[A^\pi(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t \mid s_t)\right]
\end{aligned}
$$

这就是 `Advantage Actor-Critic` 算法，通常简称为 `A2C` 算法。然而 `A2C` 算法并不是由一篇单独的论文提出来的，而是在 `Asynchronous Advantage Actor-Critic` 算法（简称 `A3C`）的论文中提到的。`A3C` 算法，顾名思义就是异步的 `A2C` 算法，在算法原理上跟 `A2C` 算法是一模一样的，只是引入了多进程的概念提高了训练效率。

<div align=center>
<img width="600" src="../figs/ch10/a3c_architecture.png"/>
</div>
<div align=center>图 10.2 A3C 算法架构</div>

如图所示，原先的 `A2C` 算法相当于只有一个全局网络并持续与环境交互更新。而 `A3C` 算法中增加了多个进程，每一个进程都拥有一个独立的网络和环境以供交互，并且每个进程每隔一段时间都会将自己的参数同步到全局网络中，这样就能提高训练效率。这种训练模式也是比较常见的多进程训练模式，也能用于其他算法中，也包括前面讲到的基于价值的算法。

## GAE 算法