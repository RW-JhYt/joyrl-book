# DQN 算法

DQN 算法，英文全称 Deep Q-learning，顾名思义就是基于深度网络模型的 Q-learning 算法，主要由 DeepMind 公司于2013年和2015年分别提出的两篇论文来实现，即《Playing Atari with Deep Reinforcement Learning》和《Human-level Control through Deep Reinforcement Learning》。DQN 算法相对于 Q-learning 算法来说更新方法本质上是一样的，而 DQN 算法最重要的贡献之一就是本章节开头讲的，用神经网络替换表格的形式来近似动作价值函数$Q(\boldsymbol{s},\boldsymbol{a})$。

<div align=center>
<img width="600" src="../figs/ch7/dqn_network.png"/>
</div>
<div align=center>图 7.1 DQN 网络结构</div>


如图 7.1 所示，在 DQN 的网络模型中，我们将当前状态$s_t$作为输入，并输出动作空间中所有动作（假设这里只有两个动作，即1和2）对应的动作价值即$Q$值，我们记做$Q(s_t,\boldsymbol{a})$。对于其他状态，该网络模型同样可以输出所有动作对应的价值，这样一来神经网络近似的动作价值函数可以表示为$Q^{\theta}(\boldsymbol{s},\boldsymbol{a})$。其中$\theta$就是神经网络模型的参数，可以结合梯度下降的方法求解。

具体该怎么结合梯度下降来更新$Q$值呢？我们首先回顾一下 Q-learning 算法的更新公式如下：

$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma\max _{a}Q(s_{t+1},a)-Q(s_t,a_t)]
$$

我们注意到公式右边两项$r_t+\gamma\max _{a}Q(s_{t+1},a)$和$Q(s_t,a_t)$分别表示期望的$Q$值和实际的$Q$值，其中预测的$Q$值是用下一个状态对应$Q$值的最大值来近似的。换句话说，在更新$Q$值并达到收敛的过程中，期望的$Q$值也应该接近实际的$Q$值，即我们希望最小化$r_t+\gamma\max _{a}Q(s_{t+1},a)$和$Q(s_t,a_t)$之间的损失，其中$\alpha$是学习率，尽管优化参数的公式跟深度学习中梯度下降法优化参数的公式有一些区别（比如增加了$\gamma$和$r_t$等参数）。从这个角度上来看，强化学习跟深度学习的训练方式其实是一样的，不同的地方在于强化学习用于训练的样本（包括状态、动作和奖励等等）是与环境实时交互得到的，而深度学习则是事先准备好的训练集。当然训练方式类似并不代表强化学习和深度学习之间的区别就很小，本质上来说强化学习和深度学习解决的问题是完全不同的，前者用于解决序列决策问题，后者用于解决静态问题例如回归、分类、识别等等。在 Q-learning 中，我们是直接优化 Q 值的，而在 DQN 中使用神经网络来近似 Q 值，我们则需要优化网络模型对应的参数$\theta$，如下：

$$
\begin{split}
    y_{i}= \begin{cases}r_{i} & \text {对于终止状态} s_{i} \\ r_{i}+\gamma \max _{a^{\prime}} Q\left(s_{i+1}, a^{\prime} ; \theta\right) & \text {对于非终止状态} s_{i}\end{cases}\\
    L(\theta)=\left(y_{i}-Q\left(s_{i}, a_{i} ; \theta\right)\right)^{2}\\
    \theta_i \leftarrow \theta_i - \alpha \nabla_{\theta_{i}} L_{i}\left(\theta_{i}\right)\\
\end{split}
$$

其中期望的Q值$y_{i}$增加了对终止状态和非终止状态的判断，这是因为当$s_t$为终止状态时，$Q(s_{t+1},a)$是不存在的（$s_{t+1}$不存在），所以需要将其置0，在 Q-learning 中其实也需要有同样的操作，只是出于简化考虑没有列出。

## 经验回放

前面讲到，强化学习的训练方式是与环境实时交互得到样本然后进行训练的，在 Q-learning 中我们是每次交互一个样本，通常包含当前状态（$state$）、当前动作（$action$）、下一个状态（$next\_state$）、是否为终止状态（$done$），这样一个样本我们一般称之为一个状态转移（transition）。这种每次只交互一个样本并更新的方式会产生两个问题，首先是在强化学习问题中样本之间的关联性过强（从当前状态到下一个状态的过渡不可能是突变的，比如我们在吃饭时中间忽然加快速度张开血盆大口猛吃也不会导致我们的肚子一下子就鼓鼓的，会有一个相对缓慢的变化过程），会导致更新的过程不够稳定并且容易陷入局部最优解。本质上来说这其实就是随机梯度下降相对于单纯梯度下降的好处，只是在强化学习中体现得更为明显，因为强化学习前后两个样本的关联性往往比监督学习更紧密，从而导致训练的不稳定。另外一个问题是我们每次只看一个样本来更新，这在 DQN 算法中的劣势会更加明显，因为 DQN 算法是基于深度神经网络模型的。在深度学习的梯度下降中，我们知道如果在训练时每次遍历整个数据集并更新一次损失函数和梯度是会保证不错的收敛性的，但是计算开销会很大，这就是前面所说的批梯度下降方法（batch gradient descent）。但如果每次只看一个样本并更新梯度，尽管速度会提上去，但是会导致收敛性能不好，容易在最优点附近徘徊，于是有了一个折中的方法，即小批量梯度下降（mini-batch gradient descent）。鉴于这两个问题， DeepMind 公司 在论文中提出了一个经验回放的概念（replay buffer），这个经验回放的功能主要包括三个方面。首先是能够缓存一定量的状态转移即样本，此时 DQN 算法并不急着更新并累积一定的初始样本，就好比我们学习做饭炒菜一样，先把颠勺、抡刀、切菜、放调料等等都先零碎地试一遍。然后是每次更新的时候随机从经验回放中取出一个小批量的样本并更新策略，注意这里的随机和小批量以便保证我们存储动作价值函数的网络模型是小批量随机梯度下降的。最后要保证经验回放是具有一定的容量限制的，太小了会导致收集到的样本具有一定的局限性，太大了会失去经验本身的意义。这就好比我们在做自我规划一样，往往会根据自身经验制定一个三年或者五年计划，如果制定的计划周期太长比如制定一个二十年计划是没有任何意义的，因为二十年间的变数太多会导致制定的计划失去效用。类似的经验回放太大容易导致智能体在更新策略时可能会使用一些比较久远的样本，根据马尔可夫过程的性质，太过久远的样本对于当前状态的参考意义不大，就好比我们制定一个二十年当上小学校长的计划，等到十年过去后发现我们中间有太多的变数而身不由己，这样一来当初制定的二十年计划大概率就流产了。而小批量的样本更新就好比在三五年计划的基础上我们制定一个每周计划，每周行动并反思然后结合三五年计划或目标调整下一周的行动，这种方式往往是最高效的。到这里，我们又不得不感叹一句，生活中处处是强化学习！

## 目标网络

在 DQN 算法中还有一个重要的技巧，就是使用了一个每隔若干步才更新的目标网络，与之相对的，会有一个每步更新的网络，即每次从经验回放中采样到样本就更新网络参数，在本书中一般称之为策略网络。策略网络和目标网络结构都是相同的，都用于近似 Q 值，在实践中每隔若干步才把每步更新的策略网络参数复制给目标网络，这样做的好处是保证训练的稳定，避免 Q值 的估计发散。举一个典型的例子，这里的目标网络好比明朝的皇帝，而策略网络相当于皇帝手下的太监，每次皇帝在做一些行政决策时往往不急着下定论，会让太监们去收集一圈情报，然后集思广益再做决策。这样做的好处是显而易见的，比如皇帝要处决一个可能受冤的犯人时，如果一个太监收集到一个情报说这个犯人就是真凶的时候，如果皇帝是一个急性子可能就当初处决了，但如果这时候另外一个太监收集了一个更有力的证据证明刚才那个太监收集到的情报不可靠并能够证明该犯人无罪时，那么此时皇帝就已经犯下了一个无法挽回的过错。换句话说，如果当前有个小批量样本导致模型对 Q 值进行了较差的过估计，如果接下来从经验回放中提取到的样本正好连续几个都这样的，很有可能导致 Q 值的发散（它的青春小鸟一去不回来了）。再打个比方，我们玩 RPG 或者闯关类游戏，有些人为了破纪录经常存档（Save）和回档（Load），简称“SL”大法。只要我出了错，我不满意我就加载之前的存档，假设不允许加载呢，就像 DQN 算法一样训练过程中会退不了，这时候是不是搞两个档，一个档每帧都存一下，另外一个档打了不错的结果再存，也就是若干个间隔再存一下，到最后用间隔若干步数再存的档一般都比每帧都存的档好些呢。当然我们也可以再搞更多个档，也就是DQN增加多个目标网络，但是对于 DQN 算法来说没有多大必要，因为多几个网络效果不见得会好很多。

## 探索策略