# PPO 算法

本章我们开始讲解强化学习中最最最泛用的 `PPO` 算法，这个算法在强化学习领域的研究和应用中有着非常重要的地位，可以说是强化学习领域的一个里程碑式的算法。`PPO` 算法的全称是 `Proximal Policy Optimization`，中文全称为近端策略优化算法，是一种基于策略梯度的强化学习算法，由 `OpenAI` 的研究人员 `Schulman` 等人在2017年提出。`PPO` 算法的主要思想是通过在策略梯度的优化过程中引入一个重要性比率来限制策略更新的幅度，从而提高算法的稳定性和收敛性。`PPO` 算法的优点在于简单、易于实现、易于调参，而且在实际应用中的效果也非常好，因此在强化学习领域得到了广泛的应用。`PPO` 的前身是 `TRPO` 算法，旨在克服 `TRPO` 算法中的一些计算上的困难和训练上的不稳定性。`TRPO` 是一种基于策略梯度的算法，它通过定义策略更新的信赖域来保证每次更新的策略不会太远离当前的策略，以避免过大的更新引起性能下降。然而，`TRPO` 算法需要解决一个复杂的约束优化问题，计算上较为繁琐。本书主要出于实践考虑，这种太复杂且几乎已经被淘汰的 `TRPO` 算法就不再赘述了，需要深入研究或者工作面试的读者可以自行查阅相关资料。 接下来将详细讲解 `PPO` 算法的原理和实现，希望能够帮助读者更好地理解和掌握这个算法。

## 重要性采样

在将 `PPO` 算法之前，我们需要铺垫一个概念，那就是重要性采样（importance sampling）。重要性采样是一种估计随机变量的期望或者概率分布的统计方法。它的原理也很简单，假设有一个函数 $f(x)$ ，需要从分布 $p(x)$ 中采样来计算其期望值，但是在某些情况下我们可能很难从 $p(x)$ 中采样，这个时候我们可以从另一个比较容易采样的分布 $q(x)$ 中采样，来间接地达到从 $p(x)$ 中采样的效果。这个过程的数学表达式如下：

$$
E_{p(x)}[f(x)]=\int_{a}^{b} f(x) \frac{p(x)}{q(x)} q(x) d x=E_{q(x)}\left[f(x) \frac{p(x)}{q(x)}\right]
$$

对于离散分布的情况，可以表达为：

$$
E_{p(x)}[f(x)]=\frac{1}{N} \sum f\left(x_{i}\right) \frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}
$$

这样一来原问题就变成了只需要从 $q(x)$ 中采样，然后计算两个分布之间的比例 $\frac{p(x)}{q(x)}$ 即可，这个比例称之为**重要性权重**。换句话说，每次从 $q(x)$ 中采样的时候，都需要乘上对应的重要性权重来修正采样的偏差，即两个分布之间的差异。当然这里可能会有一个问题，就是当 $p(x)$ 不为 $0$ 的时候，$q(x)$ 也不能为 $0$，但是他们可以同时为 $0$ ，这样 $\frac{p(x)}{q(x)}$ 依然有定义，具体的原理由于并不是很重要，因此就不展开讲解了。

通常来讲，我们把这个 $q(x)$ 叫做提议分布（`Proposal Distribution`）, 那么重要性采样对于提议分布有什么要求呢? 其实理论上 $q(x)$ 可以是任何比较好采样的分布，比如高斯分布等等，但在实际训练的过程中，聪明的读者也不难想到我们还是希望 $q(x)$ 尽可能 $p(x)$，即重要性权重尽可能接近于 $1$ 。我们可以从方差的角度来具体展开讲讲为什么需要重要性权重尽可能等于 $1$，回忆一下方差公式：

$$
Var_{x \sim p}[f(x)]=E_{x \sim p}\left[f(x)^{2}\right]-\left(E_{x \sim p}[f(x)]\right)^{2}
$$

结合重要性采样公式，我们可以得到：

$$
Var_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]=E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2}\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^{2} \\
=E_{x \sim p}\left[f(x)^{2} \frac{p(x)}{q(x)}\right]-\left(E_{x \sim p}[f(x)]\right)^{2}
$$

不难看出，当 $q(x)$ 越接近 $p(x)$ 的时候，方差就越小，也就是说重要性权重越接近于 $1$ 的时候，反之越大。

其实重要性采样也是蒙特卡洛估计的一部分，只不过它是一种比较特殊的蒙特卡洛估计，允许我们在复杂问题中利用已知的简单分布进行采样，从而避免了直接采样困难分布的问题，同时通过适当的权重调整，可以使得蒙特卡洛估计更接近真实结果。

## PPO 算法

既然重要性采样本质上是一种在某些情况下更优的蒙特卡洛估计，再结合前面 Actor-Critic 章节中我们讲到策略梯度算法的高方差主要来源于 `Actor` 的策略梯度采样估计，读者应该不难猜出 `PPO` 算法具体是优化在什么地方了。没错，`PPO` 算法的核心思想就是通过重要性采样来优化原来的策略梯度估计，其目标函数表示如下：



$$
L_{\text {clip }}(\theta)=\hat{\mathbb{E}}_{t}\left[\min \left(r_{t}(\theta) \hat{A}_{t}, \operatorname{clip}\left(r_{t}(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}\right)\right]
$$
