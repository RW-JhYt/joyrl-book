\subsection{关键词}

同策略（on-policy）：要学习的智能体和与环境交互的智能体是同一个时对应的策略。

异策略（off-policy）：要学习的智能体和与环境交互的智能体不是同一个时对应的策略。

重要性采样（important sampling）：使用另外一种分布，来逼近所求分布的一种方法，在强化学习中通常和蒙特卡洛方法结合使用，公式如下：
\begin{equation}\nonumber
    \label{eq:}
    \rint f(x) p(x) \mathrm{d} x=\rint f(x) \frac{p(x)}{q(x)} q(x) \mathrm{d} x=E_{x \sim q}[f(x){\frac{p(x)}{q(x)}}]=E_{x \sim p}[f(x)]
\end{equation}
我们在已知 $q$ 的分布后，可以使用上式计算出从 $p$ 这个分布采样 $x$ 代入 $f$ 以后得到的期望值。

近端策略优化（proximal policy optimization，PPO）：避免在使用重要性采样时由于在 $\theta$ 下的 $p_{\theta}\left(a_{t} | s_{t}\right)$ 与在  $\theta '$ 下的 $p_{\theta'}\left(a_{t} | s_{t}\right)$ 相差太多，导致重要性采样结果偏差较大而采取的算法。具体来说就是在训练的过程中增加一个限制，这个限制对应 $\theta$ 和 $\theta'$ 输出的动作的KL散度，来衡量 $\theta$ 与 $\theta'$ 的相似程度。


\subsection{习题}

\kw{5-1} 基于同策略的策略梯度有什么可改进之处？或者说其效率较低的原因在于什么？

\kw{5-2} 使用重要性采样时需要注意的问题有哪些？

\kw{5-3} 基于异策略的重要性采样中的数据是从 $\theta'$ 中采样出来的，从 $\theta$ 换成 $\theta'$ 有什么优势？

\kw{5-4} 在本节中近端策略优化中的KL散度指的是什么？


\subsection{面试题}

\kw{5-1} 友善的面试官：请问什么是重要性采样呀？

\kw{5-2} 友善的面试官：请问同策略和异策略的区别是什么？

\kw{5-3} 友善的面试官：请简述一下近端策略优化算法。其与信任区域策略优化算法有何关系呢？
