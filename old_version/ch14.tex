\appendix
\section{附录A习题解答}

\subsubsection*{第1章习题解答}

\kw{1-1}

本质上是智能体与环境的交互。具体地，当智能体在环境中得到当前时刻的状态后，其会基于此状态输出一个动作，这个动作会在环境中被执行并输出下一个状态和当前的这个动作得到的奖励。智能体在环境里存在的目标是最大化期望累积奖励。

\kw{1-2}

（1）强化学习处理的大多是序列数据，其很难像监督学习的样本一样满足独立同分布条件。
  
（2）强化学习有奖励的延迟，即智能体的动作作用在环境中时，环境对于智能体状态的奖励存在延迟，使得反馈不实时。
    
（3）监督学习有正确的标签，模型可以通过标签修正自己的预测来更新模型，而强化学习相当于一个“试错”的过程，其完全根据环境的“反馈”更新对自己最有利的动作。

\kw{1-3}
  
（1）有试错探索过程，即需要通过探索环境来获取对当前环境的理解。
  
（2）强化学习中的智能体会从环境中获得延迟奖励。
  
（3）强化学习的训练过程中时间非常重要，因为数据都是时间关联的，而不是像监督学习中的数据大部分是满足独立同分布的。
    
（4）强化学习中智能体的动作会影响它从环境中得到的反馈。

\kw{1-4} 
  
（1）算力的提升使我们可以更快地通过试错等方法来使得智能体在环境里面获得更多的信息，从而取得更大的奖励。
  
（2）我们有了深度强化学习这样一个端到端的训练方法，可以把特征提取、价值估计以及决策部分一起优化，这样就可以得到一个更强的决策网络。

\kw{1-5} 

状态是对环境的完整描述，不会隐藏环境信息。观测是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用同一个实值向量、矩阵或者更高阶的张量来表示状态和观测。

\kw{1-6} 
  
（1）策略函数，智能体会用策略函数来选取它下一步的动作，策略包括随机性策略和确定性策略。
  
（2）价值函数，我们用价值函数来对当前状态进行评估，即进入现在的状态可以对后面的奖励带来多大的影响。价值函数的值越大，说明进入该状态越有利。
  
（3）模型，其表示智能体对当前环境状态的理解，它决定系统是如何运行的。

\kw{1-7} 
  
（1）基于价值的智能体。显式学习的是价值函数，隐式地学习智能体的策略。因为这个策略是从学到的价值函数里面推算出来的。
  
（2）基于策略的智能体。其直接学习策略，即直接给智能体一个状态，它就会输出对应动作的概率。当然在基于策略的智能体里面并没有去学习智能体的价值函数。
  
（3）另外还有一种智能体，它把以上两者结合。把基于价值和基于策略的智能体结合起来就有了演员-评论员智能体。这一类智能体通过学习策略函数和价值函数以及两者的交互得到更佳的状态。

\kw{1-8} 

（1）基于策略迭代的强化学习方法，智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据该策略进行操作。强化学习算法直接对策略进行优化，使得制定的策略能够获得最大的奖励；基于价值迭代的强化学习方法，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。
  
（2）基于价值迭代的方法只能应用在离散的环境下，例如围棋或某些游戏领域，对于行为集合规模庞大或是动作连续的场景，如机器人控制领域，其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作)。
  
（3）基于价值迭代的强化学习算法有 Q-learning、Sarsa 等，基于策略迭代的强化学习算法有策略梯度算法等。
    
（4）此外，演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，从而取得更好的效果。

\kw{1-9}
    
针对是否需要对真实环境建模，强化学习可以分为有模型学习和免模型学习。有模型学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。总体来说，有模型学习相比免模型学习仅仅多出一个步骤，即对真实环境进行建模。免模型学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。免模型学习的泛化性要优于有模型学习，原因是有模型学习需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。

\kw{1-10} 
    
环境和奖励函数不是我们可以控制的，两者是在开始学习之前就已经事先确定的。我们唯一能做的事情是调整策略，使得智能体可以在环境中得到最大的奖励。另外，策略决定了智能体的行为，策略就是给一个外界的输入，然后它会输出现在应该要执行的动作。



\subsubsection*{第2章习题解答}

\kw{2-1} 

（1）首先，是有些马尔可夫过程是环状的，它并没有终点，所以我们想避免无穷的奖励。

（2）另外，我们想把不确定性也表示出来，希望尽可能快地得到奖励，而不是在未来的某个时刻得到奖励。

（3）接上一点，如果这个奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面才可以得到奖励。

（4）还有，在有些时候，折扣因子也可以设为0。当它被设为0后，我们就只关注它当前的奖励。我们也可以把它设为1，设为1表示未来获得的奖励与当前获得的奖励是一样的。

所以，折扣因子可以作为强化学习智能体的一个超参数进行调整，然后就会得到不同行为的智能体。

\kw{2-2}
  
通过矩阵求逆的过程，我们就可以把 $V$ 的解析解求出来。但是这个矩阵求逆的过程的复杂度是 $O(N^3)$ ，所以当状态非常多的时候，比如从10个状态到1000个状态，到100万个状态，那么当我们有100万个状态的时候，转移矩阵就会是一个100万乘100万的矩阵。对于这样一个大矩阵进行求逆是非常困难的，所以这种通过解析解去解的方法，只能应用在很小量的马尔可夫奖励过程中。

\kw{2-3}

（1）蒙特卡洛方法：可用来计算价值函数的值。以本书中的小船示例为例，当得到一个马尔可夫奖励过程后，我们可以从某一个状态开始，把小船放到水中，让它“随波逐流”，这样就会产生一条轨迹，从而得到一个折扣后的奖励 $g$ 。当积累该奖励到一定数量后，直接除以轨迹数量，就会得到其价值函数的值。
  
（2）动态规划方法：可用来计算价值函数的值。通过一直迭代对应的贝尔曼方程，最后使其收敛。当最后更新的状态与上一个状态区别不大的时候，通常是小于一个阈值 $\gamma$ 时，更新就可以停止。
  
（3）以上两者的结合方法：我们也可以使用时序差分学习方法，其为动态规划方法和蒙特卡洛方法的结合。

\kw{2-4} 
  
相对于马尔可夫奖励过程，马尔可夫决策过程多了一个决策过程，其他的定义与马尔可夫奖励过程是类似的。由于多了一个决策，多了一个动作，因此状态转移也多了一个条件，即执行一个动作，导致未来状态的变化，其不仅依赖于当前的状态，也依赖于在当前状态下智能体采取的动作决定的状态变化。对于价值函数，它也多了一个条件，多了一个当前的动作，即当前状态以及采取的动作会决定当前可能得到的奖励的多少。

另外，两者之间是有转换关系的。具体来说，已知一个马尔可夫决策过程以及一个策略 $\pi$ 时，我们可以把马尔可夫决策过程转换成马尔可夫奖励过程。在马尔可夫决策过程中，状态的转移函数 $P(s'|s,a)$ 是基于它的当前状态和当前动作的，因为我们现在已知策略函数，即在每一个状态，我们知道其采取每一个动作的概率，所以我们就可以直接把这个动作进行加和，就可以得到对于马尔可夫奖励过程的一个转移概率。同样地，对于奖励，我们可以把动作去掉，这样就会得到一个类似于马尔可夫奖励过程的奖励。

\kw{2-5} 
  
对于马尔可夫链，它的转移概率是直接决定的，即从当前时刻的状态通过转移概率得到下一时刻的状态值。但是对于马尔可夫决策过程，其中间多了一层动作的输出，即在当前这个状态，首先要决定采取某一种动作，再通过状态转移函数变化到另外一个状态。所以在当前状态与未来状态转移过程中多了一层决策性，这是马尔可夫决策过程与之前的马尔可夫过程的不同之处。在马尔可夫决策过程中，动作是由智能体决定的，所以多了一个组成部分，智能体会采取动作来决定未来的状态转移。

\kw{2-6} 
  
本质来说，当我们取得最佳价值函数后，我们可以通过对Q函数进行最大化，从而得到最佳价值。然后，我们直接对Q函数取一个让动作最大化的值，就可以直接得到其最佳策略。具体方法如下，

（1）穷举法（一般不使用）：假设我们有有限个状态、有限个动作可能性，那么每个状态我们可以采取 $A$ 种动作策略，那么总共就是 $|A|^{|S|}$ 个可能的策略。我们可以把他们穷举一遍，然后算出每种策略的价值函数，对比一下就可以得到最佳策略。但是这种方法的效率极低。

（2）策略迭代： 一种迭代方法，其由两部分组成，以下两个步骤一直在迭代进行，最终收敛，其过程有些类似于机器学习中的EM算法（期望-最大化算法）。第一个步骤是策略评估，即当前我们在优化这个策略 $\pi$ ，在优化过程中通过评估从而得到一个更新的策略；第二个步骤是策略提升，即取得价值函数后，进一步推算出它的Q函数，得到它的最大值。

（3）价值迭代： 我们一直迭代贝尔曼最优方程，通过迭代，其能逐渐趋向于最佳策略，这是价值迭代方法的核心。我们为了得到最佳的 $V^*$ ，对于每个状态的 $V^*$ 值，直接使用贝尔曼最优方程进行迭代，迭代多次之后它就会收敛到最佳策略及其对应的状态，这里是没有策略函数的。



\subsubsection*{第3章习题解答}

\kw{3-1} 

状态、动作、状态转移概率和奖励，分别对应$(S,A,P,R)$，后面有可能会加上折扣因子构成五元组。

\kw{3-2}

可以将强化学习的“学习”流程类比于人类的学习流程。人类学习就是尝试每一条路，并记录尝试每一条路后的最终结果。在人类尝试的过程中，其实就可以慢慢地了解到哪一条路（对应于强化学习中的状态概念）会更好。我们用价值函数 $V(s)$ 来定量表达该状态的优劣，然后用Q函数来判断在什么状态下做什么动作能够得到最大奖励，在强化学习中我们用Q函数来表示状态-动作值。

\kw{3-3}

对于环境和智能体。两者每交互一次以后，智能体都会向环境输出动作，接着环境会反馈给智能体当前时刻的状态和奖励。那么智能体此时会进行两步操作：
	
（1）使用已经训练好的Q表格，对应环境反馈的状态和奖励选取对应的动作进行输出。

（2）我们已经拥有了$(s_{t}, a_{t}, r_{t+1}, s_{t+1}, a_{t+1})$  这几个值，并直接使用 $a_{t+1}$ 更新我们的Q表格。

\kw{3-4}

Sarsa算法是Q学习算法的改进（这句话可参考论文 “On-Line Q-Learning Using Connectionist Systems”的摘要部分），详细描述如下。

（1）首先，Q学习是异策略的时序差分学习方法，而 Sarsa 算法是同策略的时序差分学习方法。

（2）其次，Sarsa算法在更新Q表格的时候所用到的 $a'$ 是获取下一个Q值时一定会执行的动作。这个动作有可能是用 $\varepsilon$-贪心方法采样出来的，也有可能是 $\mathrm{max}_Q$ 对应的动作，甚至是随机动作。

（3）但是Q学习在更新Q表格的时候所用到的Q值 $Q(S',a')$ 对应的动作不一定是下一步会执行的动作，因为下一步实际会执行的动作可能是因为进一步的探索而得到的。Q学习默认的动作不是通过行为策略来选取的，它默认 $a'$ 为最佳策略对应的动作，所以Q学习算法在更新的时候，不需要传入 $a'$ ，即 $a_{t+1}$ 。

（4）更新公式的对比（区别只在目标计算部分）。

Sarsa算法的公式：$r_{t+1}+\gamma Q(s_{t+1}, a_{t+1})$ 。

Q学习算法的公式：$r_{t+1}+\gamma \underset{a}{\max} Q\left(s_{t+1}, a\right)$ 。

总结起来，Sarsa算法实际上是用固有的策略产生 {$S,A,R,S',A'$} 这一条轨迹，然后使用 $Q(s_{t+1},a_{t+1})$ 更新原本的Q值 $Q(s_t,a_t)$ 。但是Q学习算法并不需要知道实际上选择的动作，它默认下一个动作就是Q值最大的那个动作。所以Sarsa算法的动作通常会更加“保守胆小”，而对应的Q学习算法的动作会更加“莽撞激进”。

\kw{3-5}

Sarsa算法就是一个典型的同策略算法，它只用一个 $\pi$ ，为了兼顾探索和开发，它在训练的时候会显得有点儿“胆小怕事”。它在解决悬崖寻路问题的时候，会尽可能地远离悬崖边，确保哪怕自己不小心向未知区域探索了一些，也还是处在安全区域内，不至于掉入悬崖中。

Q学习算法是一个比较典型的异策略算法，它有目标策略（target policy），用 $\pi$ 来表示。此外还有行为策略（behavior policy），用 $\mu$ 来表示。它分离了目标策略与行为策略，使得其可以大胆地用行为策略探索得到的经验轨迹来优化目标策略。这样智能体就更有可能探索到最优的策略。

比较Q学习算法和Sarsa算法的更新公式可以发现，Sarsa算法并没有选取最大值的操作。因此，Q学习算法是非常激进的，其希望每一步都获得最大的奖励；Sarsa算法则相对来说偏保守，会选择一条相对安全的迭代路线。



\subsubsection*{第4章习题解答}

\kw{4-1}

演员做的事情就是操控游戏的摇杆，比如向左、向右、开火等操作；环境就是游戏的主机，负责控制游戏的画面、控制怪物如何移动等；奖励函数就是当执行什么动作、发生什么状况的时候，我们可以得到多少分数，比如击杀一只怪兽得到20分、被对手暴击扣除10分、完成任务得到10分等。

\kw{4-2}

（1）一部分是环境的行为，即环境的函数内部的参数或内部的规则是什么形式的。 $p(s_{t+1}|s_t,a_t)$ 这一项代表的是环境，环境这一项通常是无法控制的，因为它是已经客观存在的，或者其形式是提前制定好的。
  
（2）另一部分是智能体的行为，我们能控制的是 $p_\theta(a_t|s_t)$ ，即给定一个状态 $s_t$，演员要采取什么样的动作 $a_t$ 取决于演员的参数 $\theta$，所以这部分是我们可以控制的。随着演员动作的不同，每个同样的轨迹，它会因为不同的概率从而表现出不同的行为。

\kw{4-3}

应该使用梯度上升法，因为要让期望奖励越大越好，所以是梯度上升法。梯度上升法在更新参数的时候要添加梯度信息。要进行梯度上升，我们先要计算期望奖励 $\bar{R}$ 的梯度。我们对 $\bar{R}$ 取一个梯度，这里只有 $p_{\theta}(\tau)$ 是与 $\theta$ 有关的，所以 $p_{\theta}(\tau)$ 为梯度的部分。

\kw{4-4}

策略梯度的公式如下：

$$
\begin{aligned}
E_{\tau \sim p_{\theta}(\tau)}\left[R(\tau) \nabla \log p_{\theta}(\tau)\right] &\approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(\tau^{n}\right) \\
&=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
\end{aligned}
$$

$p_{\theta}(\tau)$ 里面有两项，$p(s_{t+1}|s_t,a_t)$ 来自环境，$p_\theta(a_t|s_t)$ 来自智能体。 $p(s_{t+1}|s_t,a_t)$ 由环境决定，从而与 $\theta$ 无关，因此 $\nabla \log p(s_{t+1}|s_t,a_t) =0 $ ， $\nabla p_{\theta}(\tau)=\nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)$。

具体来说：

（1）假设在状态 $s_t$ 时执行动作 $a_t$，最后发现轨迹 $\tau$ 的奖励是正的，那我们就要增大这一项的概率，即增大在状态 $s_t$ 时执行动作 $a_t$ 的概率；

（2）反之，在状态 $s_t$ 时执行动作 $a_t$ 会导致轨迹 $\tau$ 的奖励变成负的，我们就要减小这一项的概率。

\kw{4-5}

用梯度提升来更新参数，对于原来的参数 $\theta$ ，可以将原始的 $\theta$ 加上更新梯度，再乘一个学习率。通常学习率也需要调整，与神经网络一样，我们可以使用 Adam、RMSProp、SGD 等优化器对其进行调整。
 
\kw{4-6}

（1）增加基线：为了防止所有奖励都为正，从而导致每一个状态和动作的变换，都会使得每一项变换的概率上升，我们把奖励减去一项 $b$，称之为基线。当减去 $b$ 后，就可以让奖励 $R(\tau^n)-b$ 有正有负。所以如果得到的总奖励 $R(\tau^n)$ 大于 $b$ ，就让它的概率增大。如果总奖励小于 $b$，就算它是正的，值很小也是不好的，就需要让这一项的概率减小。如果奖励 $R(\tau^n)$ 小于 $b$ ，就要让采取这个动作的奖励下降，这样也符合常理。但是使用基线会让本来奖励很大的“动作”的奖励变小，从而降低更新速率。

（2）指派合适的分数：首先，原始权重是整个回合的总奖励。现在改成从某个时间点 $t$ 开始，假设动作是在时间点 $t$ 被执行的，从时间点 $t$，一直到游戏结束所有奖励的总和大小，才真正代表这个动作是好的还是不好的；接下来我们再进一步，把未来的奖励打一个折扣，我们称由此得到的奖励的和为折扣回报。

（3）综合以上两种技巧，我们将其统称为优势函数，用 $A$ 来代表优势函数。优势函数取决于状态和动作，即我们需计算的是在某一个状态 $s$ 采取某一个动作 $a$ 的时候，优势函数有多大。

（4）优势函数的意义在于衡量假设我们在某一个状态 $s_t$ 执行某一个动作 $a_t$，相较于其他可能动作的优势。它在意的不是绝对的好，而是相对的好，即相对优势，因为会减去一个基线 $b$ 。 $A_{\theta}\left(s_{t}, a_{t}\right)$ 通常可以由一个网络预估出来，这个网络叫作评论员。

\kw{4-7}

（1）两者的更新频率不同。蒙特卡洛强化学习方法是每一个回合更新一次，即需要经历完整的状态序列后再更新，比如贪吃蛇游戏，贪吃蛇“死了”即游戏结束后再更新。而时序差分强化学习方法是每一步就更新一次，比如贪吃蛇游戏，贪吃蛇每移动一次（或几次）就进行更新。相对来说，时序差分强化学习方法比蒙特卡洛强化学习方法更新的频率更高。

（2）时序差分强化学习方法能够在知道一个小步后就进行学习，相比于蒙特卡洛强化学习方法，其更加快速和灵活。

（3）具体例如：假如我们要优化开车去公司的通勤时间。对于此问题，每一次通勤，我们将到达不同的路口。对于时序差分强化学习方法，其会对每一个经过的路口计算时间，例如在路口 A 就开始更新预计到达路口 B、路口 C $\cdots \cdots$ ，以及到达公司的时间；对于蒙特卡洛强化学习方法，其不会每经过一个路口就更新时间，而是到达最终的目的地后，再修改到达每一个路口和到达公司对应的时间。

\kw{4-8}

首先我们需要根据一个确定好的策略模型来输出每一个可能动作的概率，对于所有动作的概率，我们使用采样方法（或者是随机的方法）选择一个动作与环境进行交互，同时环境会给我们反馈整个回合的数据。将此回合数据输入学习函数中，并根据回合数据进行损失函数的构造，通过Adam等优化器的优化，再更新我们的策略模型。



\subsubsection*{第5章习题解答}

\kw{5-1}

经典策略梯度的大部分时间花在数据采样上，即当我们的智能体与环境交互后，我们就要进行策略模型的更新。但是对于一个回合我们仅能更新策略模型一次，更新完后我们就要花时间重新采样数据，然后才能再次进行如上的更新。

所以我们可以使用异策略的方法，即使用另一个不同的策略和演员，与环境进行交互并用所采样的数据进行原先策略的更新。这样等价于使用同一组数据，在同一个回合，我们对整个策略模型更新了多次，这样会更加有效率。

\kw{5-2}

我们可以在重要性采样中将 $p$ 替换为任意的 $q$，但是本质上要求两者的分布不能差太多，即使我们补偿了不同数据分布的权重 $\frac{p(x)}{q(x)}$ 。 $E_{x \sim p}[f(x)]=E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]$ ，当我们对于两者的采样次数都比较多时，最终的结果会是较为接近的。但是通常我们不会取理想数量的采样数据，所以如果两者的分布相差较大，最后结果的方差将会很大。

\kw{5-3}

使用基于异策略的重要性采样后，我们不用 $\theta$ 与环境交互，而是由另外一个策略 $\theta'$ 进行示范。 $\theta'$ 的任务就是示范给 $\theta$ 看，它和环境交互，告诉 $\theta$ 它与环境交互会发生什么事，以此来训练 $\theta$ 。我们要训练的是 $\theta$ ，$\theta'$ 只负责做示范，负责与环境交互，所以采样出来的数据与 $\theta$ 本身是没有关系的。所以就可以让 $\theta'$ 与环境交互采样大量数据，$\theta$ 可以更新参数多次。一直到 $\theta$ 训练到一定的程度、参数更新多次以后，$\theta'$ 再重新采样，这就是同策略换成异策略的妙处。

\kw{5-4}

本质来说，KL散度是一个函数，其度量的是两个动作（对应的参数分别为 $\theta$ 和 $\theta'$ ）间的行为距离，而不是参数距离。这里的行为距离可以理解为在相同状态下输出动作的差距（概率分布上的差距），概率分布即KL散度。 



\subsubsection*{第6章习题解答}

\kw{6-1}

首先深度Q网络为基于深度学习的Q学习算法，而在Q学习中，我们使用表格来存储每一个状态下动作的奖励，即我们在正文中介绍的动作价值函数 $Q(s,a)$ 。但是在我们的实际任务中，状态量通常数量巨大，并且在连续任务中会遇到维度灾难等问题，使用真正的价值函数通常是不切实际的，所以使用了与价值函数近似的表示方法。

\kw{6-2}

与状态和演员直接相关。我们在讨论输出时通常是针对一个演员衡量一个状态的好坏，也就是状态、价值从本质上来说是依赖于演员的。不同的演员在相同的状态下也会有不同的输出。

\kw{6-3}

（1）基于蒙特卡洛的方法：本质上就是让演员与环境交互。评论员根据统计结果，将演员和状态对应起来，即如果演员看到某一状态 $s_a$ ，将预测接下来的累积奖励有多大，如果看到另一个状态 $s_b$，将预测接下来的累积奖励有多大。但是其普适性不好，其需要匹配到所有的状态。如果我们面对的是一个简单的例如贪吃蛇游戏等状态有限的问题还可以应对，但是如果我们面对的是一个图片型的任务，我们几乎不可能将所有的状态（对应每一帧的图像）的都“记录”下来。总之，其不能对未出现过的输入状态进行对应价值的输出。

（2）基于蒙特卡洛的网络方法：为了弥补上面描述的基于蒙特卡洛的方法的不足，我们将其中的状态价值函数 $V_{\pi}(s)$ 定义为一个网络，其可以对于从未出现过的输入状态，根据网络的泛化和拟合能力，“估测”出一个价值输出。

（3）基于时序差分的网络方法，即基于时序差分的网络：与我们在前4章介绍的蒙特卡洛方法与时序差分方法的区别一样，基于时序差分的网络方法和基于蒙特卡洛的网络方法的区别也相同。在基于蒙特卡洛的方法中，每次我们都要计算累积奖励，也就是从某一个状态 $s_a$ 一直到游戏结束的时候，得到的所有奖励的总和。所以要应用基于蒙特卡洛的方法时，我们必须至少把游戏玩到结束。但有些游戏要玩到游戏结束才能够更新网络花费的时间太长了，因此我们会采用基于时序差分的网络方法。基于时序差分的网络方法不需要把游戏玩到结束，只要在游戏某一个状态 $s_t$ 的时候，采取动作 $a_t$ 得到奖励 $r_t$ ，进入状态 $s_{t+1}$，就可以应用基于时序差分的网络方法。其公式与之前介绍的时序差分方法类似，即 $V_{\pi}\left(s_{t}\right)=V_{\pi}\left(s_{t+1}\right)+r_{t}$。

（4）基于蒙特卡洛方法和基于时序差分方法的区别在于： 蒙特卡洛方法本身具有很大的随机性，我们可以将其 $G_a$ 视为一个随机变量，所以其最终的偏差很大。而对于时序差分，其具有随机的变量 $r$ 。因为在状态 $s_t$ 采取同一个动作，所得的奖励也不一定是一样的，所以对于时序差分方法来说，$r$ 是一个随机变量。但是相对于蒙特卡洛方法的 $G_a$ 来说，$r$ 的随机性非常小，这是因为 $G_a$ 本身就是由很多的 $r$ 组合而成的。从另一个角度来说，在时序差分方法中，我们的前提是 $r_t=V_{\pi}\left(s_{t+1}\right)-V_{\pi}\left(s_{t}\right)$ ，但是我们通常无法保证 $V_{\pi}\left(s_{t+1}\right)$ 、$V_{\pi}\left(s_{t}\right)$ 计算的误差为0。所以当 $V_{\pi}\left(s_{t+1}\right)$ 、$V_{\pi}\left(s_{t}\right)$ 计算得不准确，得到的结果也会是不准确的。总之，两者各有优劣。

（5）目前，基于时序差分的方法是比较常用的，基于蒙特卡洛的方法其实是比较少用的。

\kw{6-4}

理想状态下，我们期望对于一个输入状态，输出其无误差的奖励价值。对于价值函数，如果输入状态是 $s_a$，正确的输出价值应该是 $G_a$。如果输入状态是 $s_b$，正确的输出价值应该是 $G_b$。所以在训练的时候，其就是一个典型的机器学习中的回归问题。我们实际中需要输出的仅仅是一个非精确值，即我们希望在输入状态 $s_a$ 的时候，输出价值与 $G_a$ 越近越好；输入 $s_b$ 的时候，输出价值与 $G_b$ 越近越好。其训练方法与我们在训练卷积神经网络等深度神经网络时的方法类似。

\kw{6-5}

基于时序差分网络的核心函数为 $V_{\pi}\left(s_{t}\right)=V_{\pi}\left(s_{t+1}\right)+r_{t}$。我们将状态 $s_t$ 输入网络，因为将 $s_t$ 输入网络会得到输出 $V_{\pi}(s_t)$，同样将 $s_{t+1}$ 输入网络会得到$V_{\pi}(s_{t+1})$。同时核心函数 $V_{\pi}\left(s_{t}\right)=V_{\pi}\left(s_{t+1}\right)+r_{t}$  告诉我们， $V_{\pi}(s_t)$ 减 $V_{\pi}(s_{t+1})$ 的值应该是 $r_t$。我们希望它们两个相减的损失值与 $r_t$ 尽可能地接近。这也是网络的优化目标，我们称之为损失函数。

\kw{6-6}

（1）状态价值函数的输入是一个状态，它根据状态计算出当前这个状态以后的累积奖励的期望值是多少。

（2）动作价值函数的输入是状态-动作对，即在某一个状态采取某一个动作，同时假设我们都使用策略 $\pi$ ，得到的累积奖励的期望值是多少。

\kw{6-7}

（1）使用状态-动作对表示时，即当Q函数的输入是状态-动作对时，输出就是一个标量。

（2）仅使用状态表示时，即当Q函数的输入仅是一个状态时，输出就是多个价值。

\kw{6-8}

首先， $\pi'$ 由 $\pi^{\prime}(s)=\underset{a}{\arg \max} Q_{\pi}(s, a)$ 计算而得，其表示假设我们已经学习出 $\pi$ 的Q函数，对于某一个状态 $s$ ，把所有可能的动作 $a$ 一一代入这个Q函数，看看哪一个动作 $a$ 可以让Q函数的价值最大，那么该动作就是 $\pi'$ 将会执行的动作。所以根据以上方法决定动作的策略 $\pi'$ 一定比原来的策略 $\pi$ 要好，即 $V_{\pi^{\prime}}(s) \geqslant V_{\pi}(s)$ 。

\kw{6-9}

（1） $\varepsilon$-贪心： 我们有 $1-\varepsilon$ 的概率（通常 $\varepsilon$ 很小）完全按照Q函数决定动作，但是有 $\varepsilon$ 的概率使得动作是随机的。通常在实现上， $\varepsilon$的值会随着时间递减。也就是在最开始的时候，因为还不知道哪个动作是比较好的，所以我们会花比较大的力气做探索。接下来随着训练的次数越来越多，我们已经比较确定哪一种策略是比较好的，就会减少探索，从而把 $\varepsilon$ 的值变小，主要根据Q函数来决定未来的动作，随机性就会变小。

（2） 玻尔兹曼探索：这个方法比较像策略梯度。在策略梯度里面，网络的输出是一个期望动作空间上的一个概率分布，我们根据概率分布去采样。所以也可以根据Q值确定一个概率分布，假设某一个动作的Q值越大，代表它越好，我们采取这个动作的概率就越高。

\kw{6-10}

（1）首先，在强化学习的整个过程中，最花时间的过程是与环境交互，使用GPU乃至TPU来训练网络相对来说是比较快的。而用回放缓冲区可以减少与环境交互的次数。因为在训练的时候，我们的经验不需要通通来自于某一个策略（或者当前时刻的策略）。一些由过去的策略所得到的经验可以放在回放缓冲区中被使用多次，被反复地再利用，这样采样到的经验才能被高效地利用。

（2）另外，在训练网络的时候，我们其实希望一个批量里面的数据越多样越好。如果一个批量里面的数据都是同性质的，我们训练出的模型的拟合能力可能不会很乐观。如果一个批量里面都是一样的数据，在训练的时候，拟合效果会比较差。如果回放缓冲区里面的经验通通来自于不同的策略，那么采样到的一个批量里面的数据会是比较多样的。这样可以保证我们的模型的性能至少不会很差。

\kw{6-11}

没影响。这并不是因为过去的 $\pi$ 与现在的 $\pi'$ 很相似，就算过去的$\pi$ 不是很相似，其实也是没有关系的。主要的原因是我们并不是去采样一条轨迹，我们只能采样一个经验，所以与是不是异策略是没有关系的。就算是异策略，就算是这些经验不是来自 $\pi$，我们还是可以使用这些经验来估测 $Q_{\pi}(s,a)$。


\subsubsection*{第7章习题解答}

\kw{7-1}

因为实际应用时，需要让 $Q(s_t ,a_t)$ 与 $r_t+\max_{a}Q(s_{t+1},a)$ 尽可能相等，即与我们的目标越接近越好。可以发现，目标值很容易一不小心就被设置得太高，因为在计算该目标值的时候，我们实际上在做的事情是看哪一个动作 $a$ 可以得到最大的Q值，就把它加上去，使其成为我们的目标。
  
例如，现在有4个动作，本来它们得到的Q值都是差不多的，它们得到的奖励也都是差不多的，但是在估算的时候是有误差的。如果第1个动作被高估了，那目标就会执行该动作，然后就会选这个高估的动作的Q值加上 $r_t$ 当作目标值。如果第4个动作被高估了，那目标就会选第4个动作的Q值加上 $r_t$ 当作目标值。所以目标总是会选那个Q值被高估的动作，我们也总是会选那个奖励被高估的动作的Q值当作Q值的最大值的结果去加上 $r_t$ 当作新目标值，因此目标值总是太大。

\kw{7-2}

我们可以使用双深度Q网络解决这个问题。首先，在双深度Q网络里面，选动作的Q函数与计算价值的Q函数不同。在深度Q网络中，需要穷举所有的动作 $a$，把每一个动作 $a$ 都代入Q函数并计算哪一个动作 $a$ 反馈的Q值最大，就把这个Q值加上 $r_t$ 。但是对于双深度Q网络的两个Q网络，第一个Q网络决定哪一个动作的Q值最大，以此来决定选取的动作。我们的Q值是用 $Q'$ 算出来的，这样有什么好处呢？为什么这样就可以避免过度估计的问题呢？假设我们有两个Q函数，如果第一个Q函数高估了它现在选出来的动作 $a$ 的值，那没关系，只要第二个Q函数 $Q'$ 没有高估这个动作 $a$ 的值，计算得到的就还是正常值。假设反过来是 $Q'$ 高估了某一个动作的值，那也不会产生过度估计的问题。

\kw{7-3}

在双深度Q网络中存在两个Q网络，一个是目标的Q网络，一个是真正需要更新的Q网络。具体实现方法是使用需要更新的Q网络选动作，然后使用目标的Q网络计算价值。双深度Q网络相较于深度Q网络的更改是最少的，它几乎没有增加任何的运算量，甚至连新的网络都不需要。唯一要改变的就是在找最佳动作 $a$ 的时候，本来使用 $Q'$ 来计算，即用目标的Q网络来计算，现在改成用需要更新的Q网络来计算。

\kw{7-4}

对于 $\mathrm{Q}(s,a)$ ，其对应的状态由于为表格的形式，因此是离散的，而实际中的状态却不是离散的。对于 $\mathrm{Q}(s,a)$ 的计算公式—— $\mathrm{Q}(s,a)=\mathrm{V}(s)+\mathrm{A}(s,a)$ 。其中的 $\mathrm{V}(s)$ 对于不同的状态都有值， $\mathrm{A}(s,a)$ 对于不同的状态都有不同的动作对应的值。所以从本质上来说，我们最终矩阵 $\mathrm{Q}(s,a)$ 的结果是将每一个 $\mathrm{V}(s)$ 加到矩阵 $\mathrm{A}(s,a)$ 中得到的。从模型的角度考虑，我们的网络直接改变的不是 $\mathrm{Q}(s,a)$ ，而是改变的 $\mathrm{V}$、$\mathrm{A}$ 。但是有时我们更新时不一定会将 $\mathrm{V}(s)$ 和 $\mathrm{Q}(s,a)$ 都更新。将状态和动作对分成两个部分后，我们就不需要将所有的状态-动作对都采样一遍，我们可以使用更高效的估计Q值的方法将最终的 $\mathrm{Q}(s,a)$ 计算出来。

\kw{7-5}

优势：时序差分方法只采样了一步，所以某一步得到的数据是真实值，接下来的都是Q值估测出来的。使用蒙特卡洛和时序差分平衡方法采样比较多步，如采样$N$步才估测价值，所以估测的部分所造成的影响就会比较小。

劣势：因为智能体的奖励比较多，所以当我们把$N$步的奖励加起来时，对应的方差就会比较大。为了缓解方差大的问题，我们可以通过调整$N$值，在方差与不精确的Q值之间取得一个平衡。这里介绍的参数$N$是超参数，需要微调参数 $N$，例如是要多采样3步、还是多采样5步。



\subsubsection*{第8章习题解答}

\kw{8-1}

在深度Q网络中，只要能够估计出Q函数，就可以找到一个比较好的策略。同样地，只要能够估计出Q函数，就可以增强对应的策略。因为估计Q函数是一个比较容易的回归问题，在这个回归问题中，我们可以时刻观察模型训练的效果是不是越来越好（一般情况下我们只需要关注回归的损失有没有下降，就可以判断模型学习得好不好），所以估计Q函数相较于学习一个策略来说是比较容易的。只需要估计Q函数，就可以保证现在一定会得到比较好的策略，同样其也比较容易操作。对比来说，策略梯度方法中的优化目标是最大化总回报，但是我们很难找到一个明确的损失函数来进行优化，其本质上是一个策略搜索问题，也就是一个无约束的优化问题。

\kw{8-2}

我们在日常生活中常见的问题大都是包含连续动作的，例如智能体要进行自动驾驶，其就需要决定方向盘要左转几度或右转几度，这就是连续的动作；假设智能体是一个机器人，它身上有50个关节，它的每一个动作就对应到这50个关节的角度，这些角度也是连续的。

然而在使用深度Q网络时，很重要的一步是要求能够解决对应的优化问题。当我们预估出Q函数 $Q(s,a)$ 以后，必须要找到一个动作，它可以让 $Q(s,a)$ 最大。假设动作是离散的，那么动作 $a$ 的可能性是有限的。但如果动作是连续的，我们就不能像对离散的动作一样，穷举所有可能的动作了。

为了解决这个问题，有以下几种方案。

（1）第一个方案：我们可以使用采样方法，即随机采样出$N$个可能的动作，然后一个一个代入Q函数中，计算对应的$N$个Q值，并比较哪一个最大。但是这个方案因为使用采样方法所以不会非常精确。

（2）第二个方案：我们将这个连续动作问题，建模为一个优化问题，从而可以用梯度上升去最大化我们的目标函数。具体地，我们将动作视为变量，使用梯度上升更新动作对应的Q值。但是这个方案通常时间花销比较大，因为其需要迭代计算。

（3）第三个方案：设计一个特别的网络架构，即设计一个特别的Q函数，使得求解让Q函数最大化的动作 $a$ 变得非常容易。也就是这里的Q函数不是一个广义的Q函数，我们可以使用特殊方法设计Q函数，使得寻找让这个Q函数最大的动作 $a$ 非常容易。但是这个方案的Q函数不能随意设计，其必须有一些额外的限制。

（4）第四个方案：不用深度Q网络，毕竟用其处理连续动作比较麻烦。



\subsubsection*{第9章习题解答}

\kw{9-1}

在传统的方法中，我们有一个策略 $\pi$ 以及一个初始的演员与环境交互、收集数据以及反馈。通过每一步得到的反馈，我们进一步更新我们的策略 $\pi$ ，通常我们使用的更新方式是策略梯度。但是对于演员-评论员算法，我们不是直接使用每一步得到的数据和反馈进行策略 $\pi$ 的更新，而是使用这些数据和反馈进行价值函数的估计，这里我们通常使用的算法包括时序差分和蒙特卡洛等算法以及基于它们的优化算法。接下来我们再基于价值函数来更新策略，公式如下：

$$
\nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(r_{t}^{n}+V_{\pi}\left(s_{t+1}^{n}\right)-V_{\pi}\left(s_{t}^{n}\right)\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
$$

其中 $r_{t}^{n}+V_{\pi}\left(s_{t+1}^{n}\right)-V_{\pi}\left(s_{t}^{n}\right)$ 为优势函数。我们通过以上方法得到新的策略后，再与环境交互，然后重复预估价值函数的操作，用价值函数来更新我们的策略。以上的整个方法我们称为优势演员-评论员算法。

\kw{9-2}

（1）预估两个网络：一个是价值网络；另外一个是策略网络。价值网络的输入是一个状态，输出是一个标签；策略网络的输入是一个状态，输出是一个动作的分布。这两个网络中，演员和评论员的输入都是状态，所以它们前面几层是可以共享的。例如，玩雅达利游戏时，输入都是图片。输入的图片都非常复杂，且比较大，通常前期我们都会用一些卷积神经网络来处理这些图片，把图片抽象成深层次的特征，这些网络对演员与评论员网络来说是可以共用的。我们可以让演员与评论员的前面几层共用同一组参数，这一组参数可能是卷积神经网络中的参数。先把输入的像素变成比较高维度的特征信息，然后输入演员网络决定要采取什么样的动作，评论员网络使用价值函数计算期望奖励。

（2）探索机制：其目的是对策略 $\pi$ 的输出分布进行限制，从而使得分布的熵不要太小，即希望不同的动作被采用的概率平均一些。这样在测试的时候，智能体才会多尝试各种不同的动作，才会对环境进行充分探索，从而得到比较好的结果。

\kw{9-3}

异步优势演员-评论员算法，即算法一开始会有一个全局网络，其包含策略部分和价值部分。假设它的参数是 $\theta_1$，假设对于每一个演员都用一个CPU训练，每一个演员工作前都会将全局网络的参数复制进来。然后演员与环境进行交互，每一个演员与环境交互后，都会计算出梯度并且更新全局网络的参数。这里要注意的是，所有的演员都是并行运行的。所以每个演员都是在全局网络复制了参数以后，执行完再把参数传回去。所以当第一个演员执行完想要把参数传回去的时候，本来它要的参数是 $\theta_1$，等它把梯度传回去的时候，可能原来的参数已经被覆盖，变成 $\theta_2$ 了。
  
\kw{9-4}

（1）把 $Q(s,a)$ 换成了 $\pi$。经典的Q学习算法是用 $Q(s,a)$ 来决定在状态 $s_t$ 产生哪一个动作 $a_{t}$ ，路径衍生策略梯度是直接用 $\pi$ 来决定。面对前者，我们需要解决最大值的问题，现在的路径衍生策略梯度直接训练了一个演员网络。其输入状态 $s_t$ 就会告诉我们应该采取哪一个动作 $a_{t}$。综上，经典的Q学习算法输入状态 $s_t$，采取哪一个动作 $a_t$ 是 $Q(s,a)$ 决定的，在路径衍生策略梯度里面，我们会直接用 $\pi$ 来决定。

（2）经典的Q学习算法计算在 $s_{i+1}$ 下对应的策略采取的动作 $a$ 得到的Q值，我们会采取让 $\hat{Q}$ 最大的动作 $a$。现在的路径衍生策略梯度因为我们不需要再求解决最大化的问题，所以我们直接把状态 $s_{i+1}$ 代入策略 $\pi$ 中，就会得到在状态 $s_{i+1}$ 下，哪一个动作会带给我们最大的Q值，就执行这个动作。在Q函数中，有两个Q网络，一个是真正的Q网络，另外一个是目标Q网络。实际上在执行时，也会有两个演员网络，一个真正要学习的演员网络 $\pi$ 和一个目标演员网络 $\hat{\pi}$ 。

（3）经典的Q学习算法只需要学习Q函数，路径衍生策略梯度需要多学习一个策略 $\pi$，其目的在于最大化Q函数，希望得到的演员可以让Q函数的输出尽可能的大，这与生成对抗网络里面的生成器的概念类似。

（4）与原来的Q函数一样，我们要把目标Q网络取代掉，路径衍生策略梯度中也要把目标策略取代掉。



\subsubsection*{第10章习题解答}

\kw{10-1}

设计奖励、好奇心驱动的奖励、课程学习、逆课程学习、分层强化学习等。

\kw{10-2}

主要的问题是我们人为设计的奖励需要领域知识，需要我们自己设计出让环境与智能体更好地交互的奖励，这需要不少的经验知识，并且需要我们根据实际的效果进行调整。

\kw{10-3}

内在好奇心模块代表好奇心驱动技术中增加新的奖励函数以后的奖励函数。具体来说，其在更新计算时会考虑3个新的部分，分别是状态 $s_1$、动作 $a_1$ 和状态 $s_2$。根据 $s_1$ 、$a_1$、$a_2$，它会输出另外一个新的奖励 $r_1^i$。所以在内在好奇心模块中，我们的总奖励并不是只有 $r$ 而已，还有 $r^i$。它不是只把所有的 $r$ 相加，还把所有 $r^i$ 相加一并当作总奖励。所以，基于内在好奇心模块的智能体在与环境交互的时候，不是只希望 $r$ 越大越好，还同时希望 $r^i$ 越大越好，希望从内在好奇心模块里面得到的总奖励越大越好。

对于如何设计内在好奇心模块，其输入就像前面所说的一样，包括3部分，即现在的状态 $s_1$、在这个状态采取的动作 $a_1$、下一个状态 $s_{t+1}$，对应的输出就是奖励 $r_1^i$。输入、输出的映射是通过网络构建的，其使用状态 $s_1$ 和动作 $a_1$ 去预测下一个状态 $\hat{s}_{t+1}$ ，然后继续评判预测的状态 $\hat{s}_{t+1}$ 和真实状态 $s_{t+1}$ 的相似性，越不相似得到的奖励就越大。通俗来说这个奖励就是，如果未来的状态越难被预测，那么得到的奖励就越大。这就是好奇心机制，其倾向于让智能体做一些风险比较大的动作，从而提高其探索的能力。

同时，为了进一步增强网络的表达能力，我们通常将内在好奇心模块的输入优化为特征提取，特征提取器的输入就是状态，输出是一个特征向量，其可以表示这个状态最主要和最重要的特征，把没有意义的事物过滤。



\subsubsection*{第11章习题解答}

\kw{11-1}

行为克隆、逆强化学习或者称为逆最优控制。

\kw{11-2}

（1）首先，如果只收集专家的示范（看到某一个状态输出的动作），那么所有的结果会是非常有限的。所以我们要收集专家在各种极端状态下的动作或者说要收集更多、更复杂的数据，可以使用数据集聚合方法。

（2）另外，使用传统意义上的行为克隆，智能体会完全复制专家的行为，不管专家的行为是否合理，智能体都会硬把它记下来。智能体是一个网络，网络的容量是有限的。就算给网络足够的训练数据，它在训练数据集上得到的正确率往往也不是100\%。所以这个时候，什么该学、什么不该学就变得很重要。实际上，极少数专家的行为是没有意义的，但是使用它们的示范至少不会产生较坏的影响。

（3）还有，在进行行为克隆的时候，训练数据和测试数据往往是不匹配的。我们可以用数据集聚合来缓解这个问题。具体来说，在训练和测试的时候，数据分布是不同的。因为在强化学习中，动作会影响到接下来的状态。我们先有状态 $s_1$ ，然后采取动作 $a_1$ ，动作 $a_1$ 会决定接下来的状态 $s_2$ 。如果 $\pi^*$ 与 $\hat{\pi}$ 一模一样，那么我们训练时看到的状态与测试时看到的状态会是一样的，这样模型的泛化性能就会变得比较差。而且， $\pi^*$ 和 $\hat{\pi}$ 可能有一点儿误差，虽然这个误差在监督学习中，由于每一个样本都是独立的，因此影响不大，但对强化学习来说，可能在某个地方，也许智能体无法完全复制专家的行为，最后得到的结果就会差很多。所以行为克隆并不能够完全解决模仿学习的问题，我们可以使用另外一个比较好的方法，即逆强化学习。

\kw{11-3}

首先，我们有一个专家，其策略为 $\hat{\pi}$，这个专家负责与环境交互，给我们 $\hat{\tau_1}$ ～ $\hat{\tau_n}$，我们需要将其中的状态-动作序列都记录下来。然后对于演员，其策略为$\pi$，也需要进行一样的交互和序列的记录。接着我们需要指定一个奖励函数，并且保证专家对应的分数一定要比演员的要高，用这个奖励函数继续学习并更新我们的训练，同时套用一般条件下的强化学习方法进行演员网络的更新。在这个过程中，我们也要同时进行一开始指定的奖励函数的更新，使得演员得分越来越高，但是不超过专家的得分。最终的奖励函数应该让专家和演员对应的奖励函数都达到比较高的分数，并且从最终的奖励函数中无法分辨出两者。

\kw{11-4}

在生成对抗网络中，我们有一些比较好的图片数据集，也有一个生成器，一开始其不知道要生成什么样的图片，只能随机生成。另外，我们有一个判别器，其用来给生成的图片打分，专家生成的图片得分高，生成器生成的图片得分低。有了判别器以后，生成器会想办法去“骗”判别器。生成器希望判别器也给它生成的图片打高分。整个过程与逆强化学习的过程是类似的。我们一一对应起来看。

（1）生成的图片就是专家的判别结果，生成器就是演员，生成器会生成很多的图片并让演员与环境进行交互，从而产生很多轨迹。这些轨迹与环境交互的记录等价于生成对抗网络中的生成图片。

（2）逆强化学习中的奖励函数就是判别器。奖励函数给专家的实例打高分，给演员的交互结果打低分。

（3）考虑两者的过程，在逆强化学习中，演员会想办法从已经学习到的奖励函数中获得高分，然后迭代地循环。这个过程其实是与生成对抗网络的训练过程一致的。



\subsubsection*{第12章习题解答}

\kw{12-1}

（1）对于随机性策略 $\pi_\theta(a_t|s_t)$ ，我们输入某一个状态 $s$，采取某一个动作 $a$ 的可能性并不是百分之百的，而是有一个概率的，就好像抽奖一样，根据概率随机抽取一个动作。

（2）对于确定性策略 $\mu_{\theta}(s_t)$ ，其没有概率的影响。当神经网络的参数固定之后，输入同样的状态，必然输出同样的动作，这就是确定性策略。

\kw{12-2}

首先需要说明的是，对于连续动作的控制空间，Q学习、深度Q网络等算法是没有办法处理的，所以我们需要使用神经网络进行处理，因为其可以既输出概率值，也可以输出确定的策略 $\mu_{\theta}(s_t)$ 。

（1）要输出离散动作，最后输出的激活函数使用 Softmax 即可。其可以保证输出的是动作概率，而且所有的动作概率加和为1。

（2）要输出连续的动作，可以在输出层中加一层tanh激活函数，其可以把输出限制到 $[-1,1]$ 。我们得到这个输出后，就可以根据实际动作的一个范围再做缩放，然后将其输出给环境。比如神经网络输出一个浮点数2.8，经过tanh激活函数之后，它就可以被限制在 $[-1,1]$ ，输出0.99。假设小车的速度的动作范围是 $[-2,2]$ ，那我们就按比例将之从 $[-1,1]$ 扩大到 $[-2,2]$ ，0.99乘2，最终输出的就是1.98，将其作为小车的速度或者推小车的力输出给环境。





\section{附录B面试题解答}

\subsubsection*{第1章面试题解答}

\kw{1-1}
    
强化学习包含环境、动作和奖励3部分，其本质是智能体通过与环境的交互，使其做出的动作对应的决策得到的总奖励最大，或者说是期望最大。

\kw{1-2}
    
首先强化学习和无监督学习是不需要有标签样本的，而监督学习需要许多有标签样本来进行模型的构建和训练。其次对于强化学习与无监督学习，无监督学习直接基于给定的数据进行建模，寻找数据或特征中隐藏的结构，一般对应聚类问题；强化学习需要通过延迟奖励学习策略来得到模型与目标的距离，这个距离可以通过奖励函数进行定量判断，这里我们可以将奖励函数视为正确目标的一个稀疏、延迟形式。另外，强化学习处理的多是序列数据，样本之间通常具有强相关性，但其很难像监督学习的样本一样满足独立同分布条件。

\kw{1-3} 
    
7个字总结就是“多序列决策问题”，或者说是对应的模型未知，需要通过学习逐渐逼近真实模型的问题。并且当前的动作会影响环境的状态，即具有马尔可夫性的问题。同时应满足所有状态是可重复到达的条件，即满足可学习条件。

\kw{1-4}
    
深度学习中的损失函数的目的是使预测值和真实值之间的差距尽可能小，而强化学习中的损失函数的目的是使总奖励的期望尽可能大。

\kw{1-5}
    
我认为两者的区别主要在于是否需要对真实的环境进行建模，免模型方法不需要对环境进行建模，直接与真实环境进行交互即可，所以其通常需要较多的数据或者采样工作来优化策略，这也使其对于真实环境具有更好的泛化性能；而有模型方法需要对环境进行建模，同时在真实环境与虚拟环境中进行学习，如果建模的环境与真实环境的差异较大，那么会限制其泛化性能。现在通常使用有模型方法进行模型的构建工作。



\subsubsection*{第2章面试题解答}

\kw{2-1}
  
马尔可夫过程是一个二元组 $ <S,P> $ ， $S$ 为状态集合， $P$ 为状态转移函数；
  
马尔可夫决策过程是一个五元组 $ <S,P,A,R,\gamma> $， 其中 $R$ 表示从 $S$ 到 $S'$ 能够获得的奖励期望， $\gamma$ 为折扣因子， $A$ 为动作集合；
  
马尔可夫最重要的性质是下一个状态只与当前状态有关，与之前的状态无关，也就是 $p(s_{t+1} | s_t)= p(s_{t+1}|s_1,s_2,...,s_t)$。

\kw{2-2}
  
我们求解马尔可夫决策过程时，可以直接求解贝尔曼方程或动态规划方程：
\begin{equation}\nonumber
    \label{eq:}
    V(s)=R(S)+ \gamma \sum_{s' \in S}p(s'|s)V(s')
\end{equation}

特别地，其矩阵形式为 $\mathrm{V}=\mathrm{R}+\gamma \mathrm{PV}$。但是贝尔曼方程很难求解且计算复杂度较高，所以可以使用动态规划、蒙特卡洛以及时序差分等方法求解。

\kw{2-3}
  
如果不具备马尔可夫性，即下一个状态与之前的状态也有关，若仅用当前的状态来求解决策过程，势必导致决策的泛化能力变差。为了解决这个问题，可以利用循环神经网络对历史信息建模，获得包含历史信息的状态表征，表征过程也可以使用注意力机制等手段，最后在表征状态空间求解马尔可夫决策过程问题。

\kw{2-4} 

（1）基于状态价值函数的贝尔曼方程: $V_{\pi}(s) = \sum_{a}{\pi(a|s)}\sum_{s',r}{p(s',r|s,a)[r(s,a)+\gamma V_{\pi}(s')]}$；

（2）基于动作价值函数的贝尔曼方程: $Q_{\pi}(s,a)=\sum_{s',r}p(s',r|s,a)[r(s',a)+\gamma V_{\pi}(s')]$。

\kw{2-5}
  
最佳价值函数的定义为 $V^* (s)=\max_{\pi} V_{\pi}(s)$ ，即我们搜索一种策略 $\pi$ 来让每个状态的价值最大。$V^*$ 就是到达每一个状态其的最大价值，同时我们得到的策略就可以说是最佳策略，即 $ \pi^{*}(s)=\underset{\pi}{\arg \max }~ V_{\pi}(s) $ 。最佳策略使得每个状态的价值函数都取得最大值。所以如果我们可以得到一个最佳价值函数，就可以说某一个马尔可夫决策过程的环境被解。在这种情况下，其最佳价值函数是一致的，即其达到的上限的值是一致的，但这里可能有多个最佳策略对应于相同的最佳价值。

\kw{2-6}
  
$n$ 越大，方差越大，期望偏差越小。价值函数的更新公式如下：

$$
Q\left(S, A\right) \leftarrow Q\left(S, A\right)+\alpha\left[\sum_{i=1}^{n} \gamma^{i-1} r_{t+i}+\gamma^{n} \max _{a}   Q\left(S',a\right)-Q\left(S, A\right)\right]
$$



\subsubsection*{第3章面试题解答}

\kw{3-1}

同策略和异策略的根本区别在于生成样本的策略和参数更新时的策略是否相同。对于同策略，行为策略和要优化的策略是同一策略，更新了策略后，就用该策略的最新版本对数据进行采样；对于异策略，其使用任意行为策略来对数据进行采样，并利用其更新目标策略。例如，Q学习在计算下一状态的预期奖励时使用了最大化操作，直接选择最优动作，而当前策略并不一定能选择到最优的动作，因此这里生成样本的策略和学习时的策略不同，所以Q学习算法是异策略算法；相对应的Sarsa算法则是基于当前的策略直接执行一次动作选择，然后用动作和对应的状态更新当前的策略，因此生成样本的策略和学习时的策略相同，所以Sarsa算法为同策略算法。

\kw{3-2}

Q学习是通过计算最优动作价值函数来求策略的一种时序差分的学习方法，其更新公式为

$$
Q(s, a) \leftarrow Q(s, a) + \alpha [r(s,a) + \gamma \max_{a'} Q(s', a') - Q(s, a)]
$$

\noindent 其是异策略的，由于Q更新使用了下一个时刻的最大值，因此其只关心哪个动作使得 $Q(s_{t+1}, a)$ 取得最大值，而实际上到底采取了哪个动作（行为策略），Q学习并不关心。这表明优化策略并没有用到行为策略的数据，所以说它是异策略的。

\kw{3-3}

Sarsa算法可以算是Q学习算法的改进，其更新公式为

$$
Q(s, a) \leftarrow Q(s, a) + \alpha [r(s,a) + \gamma  Q(s', a') - Q(s, a)]
$$

\noindent 其为同策略的，Sarsa算法必须执行两次动作得到 $(s,a,r,s',a')$ 才可以更新一次；而且 $a'$ 是在特定策略 $\pi$ 的指导下执行的动作，因此估计出来的 $Q(s,a)$ 是在该策略 $\pi$ 下的Q值，样本生成用的 $\pi$ 和估计的 $\pi$ 是同一个，因此是同策略。

\kw{3-4}
	
（1）生成策略上的差异，前者确定，后者随机。基于价值的方法中动作-价值对的估计值最终会收敛（通常是不同的数，可以转化为0～1的概率），因此通常会获得一个确定的策略；基于策略的方法不会收敛到一个确定的值，另外他们会趋向于生成最佳随机策略。如果最佳策略是确定的，那么最优动作对应的值函数的值将远大于次优动作对应的值函数的值，值函数的大小代表概率的大小。

（2）动作空间是否连续，前者离散，后者连续。基于价值的方法，对于连续动作空间问题，虽然可以将动作空间离散化处理，但离散间距的选取不易确定。过大的离散间距会导致算法取不到最优动作，会在最优动作附近徘徊；过小的离散间距会使得动作的维度增大，会和高维度动作空间一样导致维度灾难，影响算法的速度。而基于策略的方法适用于连续的动作空间，在连续的动作空间中，可以不用计算每个动作的概率，而是通过正态分布选择动作。

（3）基于价值的方法，例如Q学习算法，是通过求解最优价值函数而间接地求解最优策略；基于策略的方法，例如REINFORCE等算法直接将策略参数化，通过策略搜索、策略梯度或者进化方法来更新参数以最大化回报。基于价值的方法不易扩展到连续动作空间，并且当同时采用非线性近似、自举等策略时会有收敛问题。策略梯度具有良好的收敛性。

（4）另外，对于价值迭代和策略迭代，策略迭代有两个循环，一个是在策略估计的时候，为了求当前策略的价值函数需要迭代很多次；另一个是外面的大循环，即策略评估、策略提升。价值迭代算法则是一步到位，直接估计最优价值函数，因此没有策略提升环节。

\kw{3-5}

时序差分算法是使用广义策略迭代来更新Q函数的方法，核心是使用自举，即价值函数的更新使用下一个状态的价值函数来估计当前状态的价值。也就是使用下一步的Q值 $Q(s_{t+1},a_{t+1})$ 来更新当前步的Q值 $Q(s_t,a_t) $。完整的计算公式如下：

$$
Q(s_t,a_t) \leftarrow	 Q(s_t,a_t) + \alpha [r_{t+1}+\gamma Q(s_{t+1},a_{t+1})]
$$

\kw{3-6}

蒙特卡洛方法是无偏估计，时序差分方法是有偏估计；蒙特卡洛方法的方差较大，时序差分方法的方差较小，原因在于时序差分方法中使用了自举，实现了基于平滑的效果，导致估计的价值函数的方差更小。

\kw{3-7}

相同点：都用于进行价值函数的描述与更新，并且所有方法都基于对未来事件的展望计算一个回溯值。

不同点：蒙特卡洛方法和时序差分方法属于免模型方法，而动态规划属于有模型方法；时序差分方法和蒙特卡洛方法，因为都是免模型的方法，所以对于后续状态的获知也都是基于试验的方法；时序差分方法和动态规划方法的策略评估，都能基于当前状态的下一步预测情况来得到对于当前状态的价值函数的更新。

另外，时序差分方法不需要等到试验结束后才能进行当前状态的价值函数的计算与更新，而蒙特卡洛方法需要与环境交互，产生一整条马尔可夫链并直到最终状态才能进行更新。时序差分方法和动态规划方法的策略评估不同之处为免模型和有模型，动态规划方法可以凭借已知转移概率推断出后续的状态情况，而时序差分方法借助试验才能知道。
  
蒙特卡洛方法和时序差分方法的不同在于，蒙特卡洛方法进行了完整的采样来获取长期的回报值，因而在价值估计上会有更小的偏差，但是也正因为收集了完整的信息，所以价值的方差会更大，原因在于其基于试验的采样得到，和真实的分布有差距，不充足的交互导致较大方差。而时序差分方法则相反，因为它只考虑了前一步的回报值，其他都是基于之前的估计值，因而其价值估计相对来说具有偏差大方差小的特点。

三者的联系：对于TD$(\lambda)$方法，如果 $\lambda = 0$ ，那么此时等价于时序差分方法，即只考虑下一个状态；如果 $ \lambda = 1$ ，等价于蒙特卡洛方法，即考虑 $T-1$ 个后续状态直到整个试验结束。



\subsubsection*{第4章面试题解答}

\kw{4-1}

首先我们的目的是最大化奖励函数，即调整 $\theta$ ，使得期望回报最大，可以用公式表示如下：

$$
J(\theta)=E_{\tau \sim p_{\theta(\tau)}}\left[\sum_tr(s_t,a_t)\right]
$$

其中 $\tau$ 表示从开始到结束的一条完整轨迹。通常对于最大化问题，我们可以使用梯度上升算法找到最大值，即

$$
\theta^* = \theta + \alpha\nabla J({\theta})
$$

所以我们仅仅需要计算并更新 $\nabla J({\theta})$ ，也就是计算奖励函数 $J({\theta})$ 关于 $\theta$ 的梯度，也就是策略梯度，计算方法如下：

$$
\nabla_{\theta}J(\theta) = \rint {\nabla}_{\theta}p_{\theta}(\tau)r(\tau) \mathrm{d}{\tau}=\rint p_{\theta}{\nabla}_{\theta} \mathrm{log}p_{\theta}(\tau)r(\tau)\mathrm{d}{\tau}=E_{\tau \sim p_{\theta}(\tau)}[{\nabla}_{\theta}\mathrm{log}p_{\theta}(\tau)r(\tau)]
$$

接着我们继续展开，对于 $p_{\theta}(\tau)$ ，即 $p_{\theta}(\tau|{\theta})$ ：

$$
p_{\theta}(\tau|{\theta}) = p(s_1)\prod_{t=1}^T \pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)
$$

取对数后为：

$$
\mathrm{log}p_{\theta}(\tau|{\theta}) = \mathrm{log}p(s_1)+\sum_{t=1}^T \mathrm{log}\pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)
$$

继续求导：

$$
\nabla \mathrm{log}p_{\theta}(\tau|{\theta}) = \sum_{t=1}^T \nabla_{\theta}\mathrm{log} \pi_{\theta}(a_t|s_t)
$$

代入第3个式子，可以将其化简为：

$$
\begin{aligned}
    \nabla_{\theta}J(\theta) 
    &= E_{\tau \sim p_{\theta}(\tau)}[{\nabla}_{\theta}\mathrm{log}p_{\theta}(\tau)r(\tau)] \\
    &= E_{\tau \sim p_{\theta}}[(\nabla_{\theta}\mathrm{log}\pi_{\theta}(a_t|s_t))(\sum_{t=1}^Tr(s_t,a_t))] \\
    &= \frac{1}{N}\sum_{i=1}^N[(\sum_{t=1}^T\nabla_{\theta}\mathrm{log} \pi_{\theta}(a_{i,t}|s_{i,t}))(\sum_{t=1}^Nr(s_{i,t},a_{i,t}))]    
\end{aligned}
$$

\kw{4-2}
  
（1）增加基线：为了防止所有奖励都为正，从而导致每一个状态和动作的变换，都会使得每一个变换的概率上升，我们把奖励减去一项 $b$，称 $b$ 为基线。当减去 $b$ 以后，就可以让奖励 $R(\tau^n)-b$ 有正有负。如果得到的总奖励 $R(\tau^n)$ 大于 $b$ ，就让它的概率上升。如果总奖励小于 $b$，就算它是正的，值很小也是不好的，就需要让它的概率下降。如果总奖励小于 $b$ ，就要让采取这个动作的奖励下降，这样也符合常理。但是使用基线会让本来奖励很大的“动作”的奖励变小，降低更新速率。

（2）指派合适的分数：首先，原始权重是整个回合的总奖励。现在改成从某个时间点 $t$ 开始，假设这个动作是在时间点 $t$ 被执行的，那么从时间点 $t$ ，一直到游戏结束所有奖励的总和，才真的代表这个动作是好的还是不好的；接下来我们再进一步，把未来的奖励打一个折扣，这里我们称由此得到的奖励的和为折扣回报。

（3）综合以上两种技巧，我们将其统称为优势函数，用 $A$ 来代表优势函数。优势函数取决于状态和动作，即我们需计算的是在某一个状态 $s$ 采取某一个动作 $a$ 的时候，优势函数有多大。



\subsubsection*{第5章面试题解答}

\kw{5-1}

使用另外一种分布，来逼近所求分布的一种方法，算是一种期望修正的方法，公式如下：

$$
\rint f(x) p(x) \mathrm{d} x=\rint f(x) \frac{p(x)}{q(x)} q(x) \mathrm{d} x=E_{x \sim q}[f(x){\frac{p(x)}{q(x)}}]=E_{x \sim p}[f(x)]
$$

我们在已知 $q$ 的分布后，可以使用上式计算出从 $p$ 分布的期望值。也就可以使用 $q$ 来对 $p$ 进行采样了，即重要性采样。

\kw{5-2}

我可以用一句话概括两者的区别，即生成样本的策略（价值函数）和网络参数更新时的策略（价值函数）是否相同。具体来说，同策略,生成样本的策略（价值函数）与网络更新参数时使用的策略（价值函数）相同。Sarsa算法就是同策略的，其基于当前的策略直接执行一次动作，然后用价值函数的值更新当前的策略，因此生成样本的策略和学习时的策略相同，算法为同策略算法。该算法会遭遇探索-利用窘境，仅利用目前已知的最优选择，可能学不到最优解，不能收敛到局部最优，而加入探索又降低了学习效率。 $\varepsilon$-贪心算法是这种矛盾下的折中，其优点是直接了当、速度快，缺点是不一定能够找到最优策略。异策略，生成样本的策略（价值函数）与网络更新参数时使用的策略（价值函数）不同。例如，Q学习算法在计算下一状态的预期奖励时使用了最大化操作，直接选择最优动作，而当前策略并不一定能选择到最优动作，因此这里生成样本的策略和学习时的策略不同，即异策略算法。

\kw{5-3}

近端策略优化算法借鉴了信任区域策略优化算法，通过采用一阶优化，在采样效率、算法表现以及实现和调试的复杂度之间取得了新的平衡。这是因为近端策略优化算法会在每一次迭代中尝试计算新的策略，让损失函数最小化，并且保证每一次新计算出的策略能够和原策略相差不大。换句话说，其为在避免使用重要性采样时由于在 $\theta$ 下的 $p_{\theta}\left(a_{t} | s_{t}\right)$ 与在 $\theta'$ 下的 $ p_{\theta'}\left(a_{t} | s_{t}\right) $ 差太多，导致重要性采样结果偏差较大而采取的算法。



\subsubsection*{第6章面试题解答}

\kw{6-1}

深度Q网络是基于深度学习的Q学习算法，其结合了价值函数近似与神经网络技术，并采用了目标网络和经验回放技巧进行网络的训练。

\kw{6-2}

在深度Q网络中某个动作价值函数的更新依赖于其他动作价值函数。如果我们一直更新价值网络的参数，会导致更新目标不断变化，也就是我们在追逐一个不断变化的目标，这样势必会不太稳定。为了解决基于时序差分的网络中，优化目标 $Q_{\pi}\left(s_{t}, a_{t}\right) =r_{t}+Q_{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 左右两侧会同时变化使得训练过程不稳定，从而增大回归难度的问题，目标网络选择将优化目标的右边即 $r_{t}+Q_{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 固定，通过改变优化目标左边的网络参数进行回归。对于经验回放，其会构建一个回放缓冲区，用来保存许多数据，每一个数据的内容包括：状态 $s_t$、采取的动作 $a_t$、得到的奖励 $r_t$、下一个状态 $s_{t+1}$。我们使用 $\pi$ 与环境交互多次，把收集到的数据都放到回放缓冲区中。当回放缓冲区“装满”后，就会自动删去最早进入缓冲区的数据。在训练时，对于每一轮迭代都有相对应的批量（与我们训练普通网络一样，通过采样得到），然后用这个批量中的数据去更新Q函数。即Q函数在采样和训练的时候会用到过去的经验数据，也可以消除样本之间的相关性。

\kw{6-3}

整体来说，从名称就可以看出，两者的目标价值以及价值的更新方式基本相同。但有如下不同点：

（1）首先，深度Q网络将Q学习与深度学习结合，用深度网络来近似动作价值函数，而Q学习则是采用表格进行存储。

（2）深度Q网络采用了经验回放的技巧，从历史数据中随机采样，而Q学习直接采用下一个状态的数据进行学习。

\kw{6-4}

随机性策略表示为某个状态下动作取值的分布，确定性策略在每个状态只有一个确定的动作可以选。从熵的角度来说，确定性策略的熵为0，没有任何随机性。随机性策略有利于我们进行适度的探索，确定性策略不利于进行探索。

\kw{6-5}

在神经网络中通常使用随机梯度下降法。随机的意思是我们随机选择一些样本来增量式地估计梯度，比如常用的批量训练方法。如果样本是相关的，就意味着前后两个批量很可能也是相关的，那么估计的梯度也会呈现出某种相关性。但是在极端条件下，后面的梯度估计可能会抵消掉前面的梯度估计量，从而使得训练难以收敛。



\subsubsection*{第7章面试题解答}

\kw{7-1}

深度Q网络有3个经典的变种：双深度Q网络、竞争深度Q网络、优先级双深度Q网络。
  
（1）双深度Q网络：将动作选择和价值估计分开，避免Q值被过高估计。

（2）竞争深度Q网络：将Q值分解为状态价值和优势函数，得到更多有用信息。

（3）优先级双深度Q网络：将经验池中的经验按照优先级进行采样。

\kw{7-2}

深度Q网络由于总是选择当前最优的动作价值函数来更新当前的动作价值函数，因此存在过估计问题（估计的价值函数值大于真实的价值函数值）。为了解耦这两个过程，双深度Q网络使用两个价值网络，一个网络用来执行动作选择，然后用另一个网络的价值函数对应的动作值更新当前网络。

\kw{7-3}

对于 $\boldsymbol{Q}(s,a)$ ，其对应的状态由于为表格的形式，因此是离散的，而实际的状态大多不是离散的。对于Q值 $\boldsymbol{Q}(s,a)=V(s)+\boldsymbol{A}(s,a)$ 。其中的 $V(s)$ 是对于不同的状态都有值， $\boldsymbol{A}(s,a)$ 对于不同的状态都有不同的动作对应的值。所以本质上，我们最终的矩阵 $\boldsymbol{Q}(s,a)$ 是将每一个 $V(s)$ 加到矩阵 $\boldsymbol{A}(s,a)$ 中得到的。但是有时我们更新时不一定会将 $V(s)$ 和 $\boldsymbol{Q}(s,a)$ 都更新。我们将其分成两个部分后，就不需要将所有的状态-动作对都采样一遍，我们可以使用更高效的估计Q值的方法将最终的 $\boldsymbol{Q}(s,a)$ 计算出来。



\subsubsection*{第9章面试题解答}

\kw{9-1}

A3C是异步优势演员-评论员算法，其中，评论员学习价值函数，同时有多个演员并行训练并且不时与全局参数同步。A3C旨在并行训练，是同策略算法。 

\kw{9-2}

（1）相比以价值函数为中心的算法，演员-评论员算法应用了策略梯度的技巧，这能让它在连续动作或者高维动作空间中选取合适的动作，而Q学习做这件事会很困难。

（2）相比单纯策略梯度，演员-评论员算法应用了Q学习或其他策略评估的做法，使得演员-评论员算法能进行单步更新而不是回合更新，比单纯的策略梯度的效率要高。

\kw{9-3}

下面是异步优势演员-评论员算法的大纲，由于其为异步多线程算法，我们只对其中某一单线程进行分析。


（1）定义全局参数 $\theta$ 和 $w$ 以及特定线程参数 $\theta'$ 和 $w'$。
（2）初始化时间步 $t=1$。
（3）当 $T \leqslant T_{\mathrm{max}}$。
\begin{itemize}
\item 重置梯度：$\mathrm{d} \theta = 0$ 并且 $\mathrm{d}w = 0$。
\item 将特定于线程的参数与全局参数同步：$\theta' = \theta$ 以及 $w'=w$。
\item 令 $t_{\mathrm{start}} =t$ 并且随机采样一个初始状态 $s_t$。
\item 当 （$s_t!=$ 终止状态）并且$t−t_{\mathrm{start}} \leqslant t_{\mathrm{max}}$。
\begin{itemize}
    \item 根据当前线程的策略选择当前执行的动作 $a_t\~{}\pi_{\theta'}(a_t|s_t)$，执行动作后接收奖励 $r_t$ 然后转移到下一个状态 $s_{t+1}$。
    \item 更新 $t$ 以及 $T$：$t=t+1$ 并且 $T=T+1$。
\end{itemize}
\item 初始化保存累积奖励估计值的变量。
\item 对于 $i=t_1, \dots ,t_{\mathrm{start}}$。
\begin{itemize}
    \item $r \gets \gamma r+r_i$；这里的 $r$ 是 $G_i$ 的蒙特卡洛估计。
    \item 累积关于参数 $\theta'$ 的梯度：$\mathrm{d} \theta \gets \mathrm{d}\theta + \nabla_{\theta'} \mathrm{log} \pi_{\theta'}(a_i|s_i)(r−V_{w'}(s_i))$。
    \item 累积关于参数 $w'$ 的梯度：$\mathrm{d}w \gets \mathrm{d}w+ \mathrm{\partial} (r-V_{w'}(s_i))^2 / \mathrm{\partial} w'$。
\end{itemize}
\item 分别使用 $\mathrm{d}\theta$ 以及 $\mathrm{d}w$ 异步更新 $\theta$ 以及 $w$。
\end{itemize}


\kw{9-4}

演员是策略模块，输出动作；评论员是判别器，用来计算价值函数。

\kw{9-5}

评论员衡量当前决策的好坏。结合策略模块，当评论员判别某个动作的选择是有益的时候，策略就更新参数以增大该动作出现的概率，反之减小该动作出现的概率。

\kw{9-6}

优势函数的计算公式为 $A(s,a)=Q(s,a)-V(s)=r+\gamma V(s')-V(s)$ ，其可以定量地表示选择动作 $a$ 的优势。即当动作 $a$ 低于价值函数的平均值的时候，优势函数为负值；反之为正值。其是一个标量，具体来说：

（1）如果 $A(s,a)>0$ ，梯度被推向正方向；

（2）如果 $A(s,a)<0$ ，即我们的动作比该状态下的平均值还差，则梯度被推向反方向。

这样就需要两个价值函数，所以可以使用时序差分方法做误差估计：$A(s,a)=r+\gamma V(s')-V(s)$ 。



\subsubsection*{第12章面试题解答}

\kw{12-1}

深度确定性策略梯度算法使用演员-评论员结构，但是输出的不是动作的概率，而是具体动作，其可以用于连续动作的预测。优化的目的是将深度Q网络扩展到连续的动作空间。另外，其含义如其名：

（1）深度是因为用了深度神经网络；

（2）确定性表示其输出的是一个确定的动作，可以用于连续动作的环境；

（3）策略梯度代表的是它用到的是策略网络。强化算法每个回合就会更新一次网络，但是深度确定性策略梯度算法每个步骤都会更新一次策略网络，它是一个单步更新的策略网络。

\kw{12-2}

异策略算法。（1）深度确定性策略梯度算法是优化的深度Q网络，其使用了经验回放，所以为异策略算法。（2）因为深度确定性策略梯度算法为了保证一定的探索，对输出动作加了一定的噪声，行为策略不再是优化的策略。
 
\kw{12-3}

分布的分布式深度确定性策略梯度算法（distributed distributional deep deterministic policy gradient，D4PG)，相对于深度确定性策略梯度算法，其优化部分如下。 

（1）分布式评论员：不再只估计Q值的期望值，而是估计期望Q值的分布，即将期望Q值作为一个随机变量来估计。

（2）$N$步累计回报：计算时序差分误差时，D4PG计算的是$N$步的时序差分目标值而不仅仅只有一步，这样就可以考虑未来更多步骤的回报。

（3）多个分布式并行演员：D4PG使用$K$个独立的演员并行收集训练数据并存储到同一个回放缓冲区中。

（4）优先经验回放（prioritized experience replay，PER）：使用一个非均匀概率从回放缓冲区中进行数据采样。
