\subsection{关键词}

强化学习（reinforcement learning，RL）：智能体可以在与复杂且不确定的环境进行交互时，尝试使所获得的奖励最大化的算法。

动作（action）: 环境接收到的智能体基于当前状态的输出。

状态（state）：智能体从环境中获取的状态。

奖励（reward）：智能体从环境中获取的反馈信号，这个信号指定了智能体在某一步采取了某个策略以后是否得到奖励，以及奖励的大小。

探索（exploration）：在当前的情况下，继续尝试新的动作。其有可能得到更高的奖励，也有可能一无所有。

开发（exploitation）：在当前的情况下，继续尝试已知的可以获得最大奖励的过程，即选择重复执行当前动作。

深度强化学习（deep reinforcement learning）：不需要手动设计特征，仅需要输入状态就可以让系统直接输出动作的一个端到端（end-to-end）的强化学习方法。通常使用神经网络来拟合价值函数（value function）或者策略网络（policy network）。

全部可观测（full observability）、完全可观测（fully observed）和部分可观测（partially observed）：当智能体的状态与环境的状态等价时，我们就称这个环境是全部可观测的；当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的；一般智能体不能观察到环境的所有状态时，我们称这个环境是部分可观测的。

部分可观测马尔可夫决策过程（partially observable Markov decision process，POMDP）：即马尔可夫决策过程的泛化。部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是其假设智能体无法感知环境的状态，只能知道部分观测值。

动作空间（action space）、离散动作空间（discrete action space）和连续动作空间（continuous action space）：在给定的环境中，有效动作的集合被称为动作空间，智能体的动作数量有限的动作空间称为离散动作空间，反之，则被称为连续动作空间。

基于策略的（policy-based）：智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。

基于价值的（valued-based）：智能体不需要制定显式的策略，它维护一个价值表格或者价值函数，并通过这个价值表格或价值函数来执行使得价值最大化的动作。

有模型（model-based）结构：智能体通过学习状态的转移来进行决策。

免模型（model-free）结构：智能体没有直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数或者策略网络进行决策。


\subsection{习题}

\kw{1-1} 强化学习的基本结构是什么？

\kw{1-2} 强化学习相对于监督学习为什么训练过程会更加困难？

\kw{1-3} 强化学习的基本特征有哪些？ 

\kw{1-4} 近几年强化学习发展迅速的原因有哪些？

\kw{1-5} 状态和观测有什么关系？

\kw{1-6} 一个强化学习智能体由什么组成？

\kw{1-7} 根据强化学习智能体的不同，我们可以将其分为哪几类？

\kw{1-8} 基于策略迭代和基于价值迭代的强化学习方法有什么区别？

\kw{1-9} 有模型学习和免模型学习有什么区别？

\kw{1-10} 如何通俗理解强化学习？


\subsection{面试题} 

\kw{1-1} 友善的面试官: 看来你对于强化学习还是有一定了解的呀，那么可以用一句话谈一下你对于强化学习的认识吗？

\kw{1-2} 友善的面试官: 请问，你认为强化学习、监督学习和无监督学习三者有什么区别呢？

\kw{1-3} 友善的面试官: 根据你的理解，你认为强化学习的使用场景有哪些呢？

\kw{1-4} 友善的面试官: 请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？

\kw{1-5} 友善的面试官: 你了解有模型和免模型吗？两者具体有什么区别呢？
