\subsection{关键词}

策略（policy）：在每一个演员中会有对应的策略，这个策略决定了演员的后续动作。具体来说，策略就是对于外界的输入，输出演员现在应该要执行的动作。一般地，我们将策略写成 $\pi$ 。

回报（return）：一个回合（episode）或者试验（trial）得到的所有奖励的总和，也被人们称为总奖励（total reward）。一般地，我们用 $R$ 来表示它。

轨迹（trajectory）：一个试验中我们将环境输出的状态 $s$ 与演员输出的动作 $a$ 全部组合起来形成的集合称为轨迹，即 $\tau=\left\{s_{1}, a_{1}, s_{2}, a_{2}, \cdots, s_{t}, a_{t}\right\}$ 。

奖励函数（reward function）：用于反映在某一个状态采取某一个动作可以得到的奖励分数，这是一个函数。即给定一个状态-动作对 ($s_1$,$a_1$) ，奖励函数可以输出 $r_1$ 。给定 ($s_2$,$a_2$)，它可以输出 $r_2$。 把所有的 $r$ 都加起来，我们就得到了 $R(\tau)$ ，它代表某一个轨迹 $\tau$ 的奖励。

期望奖励（expected reward）：$\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)=E_{\tau \sim p_{\theta}(\tau)}[R(\tau)]$。

REINFORCE：基于策略梯度的强化学习的经典算法，其采用回合更新的模式。


\subsection{习题}

\kw{4-1} 如果我们想让机器人自己玩视频游戏，那么强化学习中的3个组成部分（演员、环境、奖励函数）具体分别代表什么？

\kw{4-2} 在一个过程中，一个具体的轨迹{$s_1 , a_1 , s_2 , a_2$}出现的概率取决于什么？

\kw{4-3} 当我们最大化期望奖励时，应该使用什么方法？

\kw{4-4} 我们应该如何理解策略梯度的公式呢？

\kw{4-5} 我们可以使用哪些方法来进行梯度提升的计算？

\kw{4-6} 进行基于策略梯度的优化的技巧有哪些？

\kw{4-7} 对于策略梯度的两种方法，蒙特卡洛强化学习和时序差分强化学习两种方法有什么联系和区别？

\kw{4-8} 请详细描述REINFORCE算法的计算过程。


\subsection{面试题}

\kw{4-1} 友善的面试官：同学来吧，给我手动推导一下策略梯度公式的计算过程。

\kw{4-2} 友善的面试官：可以说一下你所了解的基于策略梯度优化的技巧吗？

