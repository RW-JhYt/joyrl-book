\subsection{关键词}

概率函数和奖励函数：概率函数定量地表达状态转移的概率，其可以表现环境的随机性。但是实际上，我们经常处于一个未知的环境中，即概率函数和奖励函数是未知的。

Q表格：其表示形式是表格，其中表格的横轴为动作（智能体的动作），纵轴为环境的状态，每一个坐标点对应某时刻智能体和环境的状态，并通过对应的奖励反馈选择被执行的动作。一般情况下，Q表格是一个已经训练好的表格，不过我们也可以每执行一步，就对Q表格进行更新，然后用下一个状态的Q值来更新当前状态的Q值（即时序差分方法）。

时序差分（temporal difference，TD）方法：一种Q函数（Q值）的更新方式，流程是使用下一步的Q值 $Q(s_{t+1},a_{t+1})$ 来更新当前步的Q值 $Q(s_t,a_t)$。完整的计算公式如下：$Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha [r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$ 。

Sarsa算法：一种更新前一时刻状态的单步更新的强化学习算法，也是一种同策略学习算法。该算法由于每次更新Q函数时需要知道前一步的状态、动作、奖励以及当前时刻的状态、将要执行的动作，即 $s_{t}$、$a_{t}$、$r_{t+1}$、$s_{t+1}$、$a_{t+1}$ 这几个值，因此被称为 Sarsa 算法。智能体每进行一次循环，都会用 $s_{t}$、$a_{t}$、$r_{t+1}$、$s_{t+1}$、$a_{t+1}$ 对前一步的Q值（函数）进行一次更新。


\subsection{习题}

\kw{3-1} 构成强化学习的马尔可夫决策过程的四元组有哪些变量？

\kw{3-2} 请通俗地描述强化学习的“学习”流程。

\kw{3-3} 请描述基于Sarsa算法的智能体的学习过程。

\kw{3-4} Q学习算法和Sarsa算法的区别是什么？
	
\kw{3-5} 同策略和异策略的区别是什么？
 

\subsection{面试题}

\kw{3-1} 友善的面试官：同学，你能否简述同策略和异策略的区别呢？

\kw{3-2} 友善的面试官：能否细致地讲一下Q学习算法，最好可以写出其 $Q(s,a)$ 的更新公式。另外，它是同策略还是异策略，原因是什么呢？

\kw{3-3} 友善的面试官：好的，看来你对于Q学习算法很了解，那么能否讲一下与Q学习算法类似的Sarsa算法呢，最好也可以写出其对应的 $Q(s,a)$ 更新公式。另外，它是同策略还是异策略，为什么？

\kw{3-4} 友善的面试官：请问基于价值的方法和基于策略的方法的区别是什么？

\kw{3-5} 友善的面试官：请简述一下时序差分方法。

\kw{3-6} 友善的面试官：请问蒙特卡洛方法和时序差分方法是无偏估计吗？另外谁的方差更大呢？为什么？

\kw{3-7} 友善的面试官：能否简单说一下动态规划方法、蒙特卡洛方法和时序差分方法的异同点？
