\subsection{关键词}

优势演员-评论员（advantage actor-critic，A2C）算法：一种改进的演员-评论员（actor-critic）算法。

异步优势演员-评论员（asynchronous advantage actor-critic，A3C）算法：一种改进的演员-评论员算法，通过异步的操作，实现强化学习模型训练的加速。

路径衍生策略梯度（pathwise derivative policy gradient）：一种使用Q学习来求解连续动作的算法，也是一种演员-评论员算法。其会对演员提供价值最大的动作，而不仅仅是提供某一个动作的好坏程度。


\subsection{习题}

\kw{9-1} 完整的优势演员-评论员算法的工作流程是怎样的？

\kw{9-2} 在实现演员-评论员算法的时候有哪些技巧？

\kw{9-3} 异步优势演员-评论员算法在训练时有很多的进程进行异步的工作，最后再将他们所获得的“结果”集合到一起。那么其具体是如何运作的呢？

\kw{9-4} 对比经典的Q学习算法，路径衍生策略梯度有哪些改进之处？

 
\subsection{面试题}

\kw{9-1} 友善的面试官：请简述一下异步优势演员-评论员算法（A3C），另外A3C是同策略还是异策略的模型呀？

\kw{9-2} 友善的面试官：请问演员-评论员算法有何优点呢？

\kw{9-3} 友善的面试官：请问异步优势演员-评论员算法具体是如何异步更新的？

\kw{9-4} 友善的面试官：演员-评论员算法中，演员和评论员两者的区别是什么？

\kw{9-5} 友善的面试官：演员-评论员算法框架中的评论员起了什么作用？

\kw{9-6} 友善的面试官：简述异步优势演员-评论员算法的优势函数。
  
