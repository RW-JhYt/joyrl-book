\subsection{关键词}

深度Q网络（deep Q-network，DQN）：基于深度学习的Q学习算法，其结合了价值函数近似（value function approximation）与神经网络技术，并采用目标网络和经验回放等方法进行网络的训练。

状态-价值函数（state-value function）：其输入为演员某一时刻的状态，输出为一个标量，即当演员在对应的状态时，预期的到过程结束时间段内所能获得的价值。

状态-价值函数贝尔曼方程（state-value function Bellman equation）：基于状态-价值函数的贝尔曼方程，它表示在状态 $s_t$ 下对累积奖励 $G_t$ 的期望。

Q函数（Q-function）: 其也被称为动作价值函数（action-value function）。其输入是一个状态-动作对，即在某一具体的状态采取对应的动作，假设我们都使用某个策略 $\pi$ ，得到的累积奖励的期望值有多大。

目标网络（target network）：其可解决在基于时序差分的网络中，优化目标 $Q_{\pi}\left(s_{t}, a_{t}\right) = r_{t}+Q_{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 左右两侧会同时变化使得训练过程不稳定，从而增大回归的难度的问题。目标网络选择将右边部分，即 $r_{t}+Q_{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 固定，通过改变左边部分，即 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 中的参数进行回归，这也是深度Q网络应用中比较重要的技巧。

探索（exploration）：我们在使用Q函数的时候，我们的策略完全取决于Q函数，这有可能导致出现对应的动作是固定的某几个数值的情况，而不像策略梯度中的输出是随机的，我们再从随机分布中采样选择动作。这会导致我们继续训练的输入值一样，从而“加重”输出的固定性，导致整个模型的表达能力急剧下降，这就是探索-利用窘境（exploration-exploitation dilemma）问题。我们可以使用 $\varepsilon$-贪心和玻尔兹曼探索（Boltzmann exploration）等探索方法进行优化。

经验回放（experience replay）：其会构建一个回放缓冲区（replay buffer）来保存许多经验，每一个经验的形式如下：在某一个状态 $s_t$，采取某一个动作 $a_t$，得到奖励 $r_t$，然后进入状态 $s_{t+1}$。我们使用 $\pi$ 与环境交互多次，把收集到的经验都存储在回放缓冲区中。当我们的缓冲区“装满”后，就会自动删去最早进入缓冲区的经验。在训练时，对于每一轮迭代都有相对应的批量（batch）（与我们训练普通的网络一样，都是通过采样得到的），然后用这个批量中的经验去更新我们的Q函数。综上，Q函数在采样和训练的时候，会用到过去的经验，所以这里称这个方法为经验回放，其也是深度Q网络应用中比较重要的技巧。


\subsection{习题}

\kw{6-1} 为什么在深度Q网络中采用价值函数近似的表示方法？

\kw{6-2} 评论员的输出通常与哪几个值直接相关？

\kw{6-3} 我们通常怎么衡量状态价值函数 $V_{\pi}(s)$ ？其优势和劣势分别有哪些？

\kw{6-4} 基于本章正文介绍的基于蒙特卡洛的网络方法，我们怎么训练模型呢？或者我们应该将其看作机器学习中什么类型的问题呢？

\kw{6-5} 基于本章正文中介绍的基于时序差分的网络方法，具体地，我们应该怎么训练模型呢？

\kw{6-6} 动作价值函数和状态价值函数的有什么区别和联系？

\kw{6-7} 请介绍Q函数的两种表示方法。

\kw{6-8} 当得到了Q函数后，我们应当如何找到更好的策略 $\pi'$ 呢？或者说 $\pi'$ 的本质是什么？

\kw{6-9} 解决探索-利用窘境问题的探索的方法有哪些？

\kw{6-10} 我们使用经验回放有什么好处？

\kw{6-11} 在经验回放中我们观察 $\pi$ 的价值，发现里面混杂了一些不是 $\pi$ 的经验，这会有影响吗？

  
\subsection{面试题}

\kw{6-1} 友善的面试官：请问深度Q网络是什么？其两个关键性的技巧分别是什么？

\kw{6-2} 友善的面试官：那我们继续分析！你刚才提到的深度Q网络中的两个技巧————目标网络和经验回放，其具体作用是什么呢？

\kw{6-3} 友善的面试官：深度Q网络和Q学习有什么异同点？

\kw{6-4} 友善的面试官：请问，随机性策略和确定性策略有什么区别吗？

\kw{6-5} 友善的面试官：请问不打破数据相关性，神经网络的训练效果为什么就不好？
