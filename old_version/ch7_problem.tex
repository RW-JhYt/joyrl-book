\subsection{关键词}

双深度Q网络（double DQN）：在双深度Q网络中存在两个Q网络，第一个Q网络决定哪一个动作的Q值最大，从而决定对应的动作。另一方面，Q值是用 $Q'$ 计算得到的，这样就可以避免过度估计的问题。具体地，假设我们有两个Q函数并且第一个Q函数高估了它现在执行的动作 $a$ 的值，这没关系，只要第二个Q函数 $Q'$ 没有高估动作 $a$ 的值，那么计算得到的就还是正常的值。

竞争深度Q网络（dueling DQN）：将原来的深度Q网络的计算过程分为两步。第一步计算一个与输入有关的标量 $\mathrm{V(s)}$；第二步计算一个向量 $\mathrm{A(s,a)}$ 对应每一个动作。最后的网络将两步的结果相加，得到我们最终需要的Q值。用一个公式表示就是 $\mathrm{Q(s,a)=V(s)+A(s,a)}$ 。另外，竞争深度Q网络，使用状态价值函数与动作价值函数来评估Q值。

优先级经验回放（prioritized experience replay，PER）：这个方法是为了解决我们在第6章中提出的经验回放方法的不足而提出的。我们在使用经验回放时，均匀地取出回放缓冲区（reply buffer）中的采样数据，这里并没有考虑数据间的权重大小。但是我们应该将那些训练效果不好的数据对应的权重加大，即其应该有更大的概率被采样到。综上，优先级经验回放不仅改变了被采样数据的分布，还改变了训练过程。

噪声网络（noisy net）：其在每一个回合开始的时候，即智能体要和环境交互的时候，在原来的Q函数的每一个参数上加上一个高斯噪声（Gaussian noise），把原来的Q函数变成 $\tilde{Q}$ ，即噪声Q函数。同样，我们把每一个网络的权重等参数都加上一个高斯噪声，就得到一个新的网络 $\tilde{Q}$ 。我们会使用这个新的网络与环境交互直到结束。

分布式Q函数（distributional Q-function）：对深度Q网络进行模型分布，将最终网络的输出的每一类别的动作再进行分布操作。

彩虹（rainbow）：将第6、7章7个技巧综合起来的方法，7个技巧分别是——深度Q网络、双深度Q网络、优先级经验回放的双深度Q网络、竞争深度Q网络、异步优势演员-评论员算法（A3C）、分布式Q函数、噪声网络，进而考察每一个技巧的贡献度或者与环境的交互是否是正反馈的。


\subsection{习题}

\kw{7-1} 为什么传统的深度Q网络的效果并不好？可以参考其公式 $Q(s_t ,a_t)=r_t+\max_{a}Q(s_{t+1},a)$ 来描述。

\kw{7-2} 在传统的深度Q网络中，我们应该怎么解决目标值太大的问题呢？

\kw{7-3} 请问双深度Q网络中所谓的 $Q$ 与 $Q'$ 两个网络的功能是什么？

\kw{7-4} 如何理解竞争深度Q网络的模型变化带来的好处？

\kw{7-5} 使用蒙特卡洛和时序差分平衡方法的优劣分别有哪些？


\subsection{面试题}

\kw{7-1} 友善的面试官：深度Q网络都有哪些变种？引入状态奖励的是哪种？

\kw{7-2} 友善的面试官：请简述双深度Q网络原理。

\kw{7-3} 友善的面试官：请问竞争深度Q网络模型有什么优势呢？