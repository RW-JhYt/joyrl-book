\subsection{关键词}

马尔可夫性质（Markov property，MP）: 如果某一个过程未来的状态与过去的状态无关，只由现在的状态决定，那么其具有马尔可夫性质。换句话说，一个状态的下一个状态只取决于它的当前状态，而与它当前状态之前的状态都没有关系。

马尔可夫链（Markov chain）: 概率论和数理统计中具有马尔可夫性质且存在于离散的指数集（index set）和状态空间（state space）内的随机过程（stochastic process）。

状态转移矩阵（state transition matrix）: 状态转移矩阵类似于条件概率（conditional probability），其表示当智能体到达某状态后，到达其他所有状态的概率。矩阵的每一行描述的是从某节点到达所有其他节点的概率。

马尔可夫奖励过程（Markov reward process，MRP）： 本质是马尔可夫链加上一个奖励函数。在马尔可夫奖励过程中，状态转移矩阵和它的状态都与马尔可夫链的一样，只多了一个奖励函数。奖励函数是一个期望，即在某一个状态可以获得多大的奖励。

范围（horizon）: 定义了同一个回合（episode）或者一个完整轨迹的长度，它是由有限个步数决定的。

回报（return）: 把奖励进行折扣（discounted），然后获得的对应的奖励。

贝尔曼方程（Bellman equation）: 其定义了当前状态与未来状态的迭代关系，表示当前状态的价值函数可以通过下个状态的价值函数来计算。贝尔曼方程因其提出者、动态规划创始人理查德 $\cdot$ 贝尔曼（Richard Bellman）而得名，同时也被叫作“动态规划方程”。贝尔曼方程即 $V(s)=R(s)+ \gamma \sum_{s' \in S}P(s'|s)V(s')$ ，特别地，其矩阵形式为 $\mathrm{V}=\mathrm{R}+\gamma \mathrm{PV}$。

蒙特卡洛算法（Monte Carlo algorithm，MC algorithm）： 可用来计算价值函数的值。使用本节中小船的例子，当得到一个马尔可夫奖励过程后，我们可以从某一个状态开始，把小船放到水中，让它随波流动，这样就会产生一个轨迹，从而得到一个折扣后的奖励 $g$ 。当积累该奖励到一定数量后，用它直接除以轨迹数量，就会得到其价值函数的值。

动态规划算法（dynamic programming，DP）： 其可用来计算价值函数的值。通过一直迭代对应的贝尔曼方程，最后使其收敛。当最后更新的状态与上一个状态差距不大的时候，动态规划算法的更新就可以停止。

Q函数（Q-function）： 其定义的是某一个状态和某一个动作所对应的有可能得到的回报的期望。

马尔可夫决策过程中的预测问题：即策略评估问题，给定一个马尔可夫决策过程以及一个策略 $\pi$ ，计算它的策略函数，即每个状态的价值函数值是多少。其可以通过动态规划算法解决。

马尔可夫决策过程中的控制问题：即寻找一个最佳策略，其输入是马尔可夫决策过程，输出是最佳价值函数（optimal value function）以及最佳策略（optimal policy）。其可以通过动态规划算法解决。

最佳价值函数：搜索一种策略 $\pi$ ，使每个状态的价值最大，$V^*$ 就是到达每一个状态的极大值。在极大值中，我们得到的策略是最佳策略。最佳策略使得每个状态的价值函数都取得最大值。所以当我们说某一个马尔可夫决策过程的环境可解时，其实就是我们可以得到一个最佳价值函数。


\subsection{习题}

\kw{2-1} 为什么在马尔可夫奖励过程中需要有折扣因子？

\kw{2-2} 为什么矩阵形式的贝尔曼方程的解析解比较难求得？

\kw{2-3} 计算贝尔曼方程的常见方法有哪些，它们有什么区别？

\kw{2-4} 马尔可夫奖励过程与马尔可夫决策过程的区别是什么？

\kw{2-5} 马尔可夫决策过程中的状态转移与马尔可夫奖励过程中的状态转移的结构或者计算方面的差异有哪些？

\kw{2-6} 我们如何寻找最佳策略，寻找最佳策略方法有哪些？


\subsection{面试题} 

\kw{2-1} 友善的面试官: 请问马尔可夫过程是什么？马尔可夫决策过程又是什么？其中马尔可夫最重要的性质是什么呢？

\kw{2-2} 友善的面试官: 请问我们一般怎么求解马尔可夫决策过程？

\kw{2-3} 友善的面试官: 请问如果数据流不具备马尔可夫性质怎么办？应该如何处理？

\kw{2-4} 友善的面试官: 请分别写出基于状态价值函数的贝尔曼方程以及基于动作价值函数的贝尔曼方程。

\kw{2-5} 友善的面试官: 请问最佳价值函数 $V^*$ 和最佳策略 $\pi^*$ 为什么等价呢？

\kw{2-6} 友善的面试官：能不能手写一下第$n$步的价值函数更新公式呀？另外，当 $n$ 越来越大时，价值函数的期望和方差是分别变大还是变小呢？
